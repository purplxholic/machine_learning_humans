{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "y is the sequence of tags while x is the observed sequence of words \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "#read the training data file \n",
    "def read_file_train(fp):\n",
    "    print(\"reading train file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "#read the test data file \n",
    "def read_file_test(fp):\n",
    "    print(\"reading tests file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "def write_file(fp,fn,data):\n",
    "    print(\"writing file...\")\n",
    "    with open(fp+fn, 'w',encoding='utf8') as f:\n",
    "        for k in data:\n",
    "#             print(\"data received:\")\n",
    "#             print(k[0] + ' ' + k[1]+'\\n')\n",
    "            if k[0] == \"Blank\": #replace the blanks\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(k[0] + ' ' + k[1]+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    print(\"building count(y->x) & count(y)...\")\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    "    ''' \n",
    "    '''\n",
    "    1.  from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "    '''\n",
    "    huge_count_y_x = {} #dict to contain all count(y->x)\n",
    "    huge_count_y = {} #dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x,y = d.split(\" \") #split by spaces \n",
    "        temp_tuple = (x,y)\n",
    "        \n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1 \n",
    "        \n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] =  1\n",
    "        else: \n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x,huge_count_y \n",
    "\n",
    "#part 2 first part \n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    print(\"calculating emission probability\")\n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission ={} #dict to contain all the emission probabilities \n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        \n",
    "        emission = 0.0 #if none found, then return as 0 \n",
    "        tag = seq[1]\n",
    "#         print(k,v)\n",
    "        count_y = huge_count_y[tag] \n",
    "        emission = count_y_x / count_y\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "# parsed_train = read_file_train(\"EN/train\")\n",
    "# print(calculate_emmision(parsed_train))\n",
    "\n",
    "#part 2 second part \n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "def modify_train_set(k,train_data):\n",
    "    print(\"modifying to #UNK#\")\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "#         print(td)\n",
    "        word,tag = td.rsplit(\" \",1) \n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    #identify which words appeared less than k times \n",
    "    list_of_works_less_than_k = [] \n",
    "    for w,count in count_dict.items():\n",
    "        if count < k and w != \"Blank\": #Skip blanks. \n",
    "            list_of_works_less_than_k.append(w)\n",
    "    #now replace the entries in the training set  \n",
    "    for td in train_data:\n",
    "        word,tag = td.rsplit(\" \",1)\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "                \n",
    "#part 2 third part \n",
    "'''\n",
    "using the emission data, predict the possible tags (?) \n",
    "the tuple (x,y) is key to occurance of e(x|y) \n",
    "TODO:\n",
    "1. replace with #UNK# \n",
    "2. for each word w in the test_data\n",
    "    3. find the tuples in huge_emissison that matches w\n",
    "    4. if there's more than one, select the one with max(emission probability)\n",
    "    5. if there's only 1 result, then use that tag \n",
    "    6. append the tag to y_star\n",
    "7. write the results of w, tag into a tuple\n",
    "8. return the array of tuples\n",
    "'''\n",
    "def simple_sentiment_analysis(test_data,modified_train_data,huge_emission):\n",
    "    print(\"doing simple_sentiment_analysis...\")\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.rsplit(\" \",1)\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"Blank\": #Avoid blanks.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    y_star=[] #the tags, to be appended here \n",
    "    for w in test_data:\n",
    "        temp_dict = {} # to hold the possible results found from huge_emission \n",
    "        for key,e_value in huge_emission.items():\n",
    "            #TODO: FIX THE SPACE MESS if it matches the word\n",
    "            if w == key[0]: \n",
    "                temp_dict[key] = e_value\n",
    "        #now return the tag that has the max e probability\n",
    "\n",
    "        best_result = max(temp_dict, key=temp_dict.get)[1]\n",
    "\n",
    "        #append a tupple for word, tag\n",
    "\n",
    "        y_star.append((w,best_result))\n",
    "    #return the dictionary -- don't do dic cos will remove duplicates \n",
    "\n",
    "    return y_star\n",
    "\n",
    "#part 3 first part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading train file...\n",
      "modifying to #UNK#\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability\n",
      "reading tests file...\n",
      "doing simple_sentiment_analysis...\n",
      "writing file...\n",
      "reading train file...\n",
      "modifying to #UNK#\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability\n",
      "reading tests file...\n",
      "doing simple_sentiment_analysis...\n",
      "writing file...\n",
      "reading train file...\n",
      "modifying to #UNK#\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability\n",
      "reading tests file...\n",
      "doing simple_sentiment_analysis...\n",
      "writing file...\n",
      "reading train file...\n",
      "modifying to #UNK#\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability\n",
      "reading tests file...\n",
      "doing simple_sentiment_analysis...\n",
      "writing file...\n"
     ]
    }
   ],
   "source": [
    "#code to run the file \n",
    "PART2_FN = \"dev.p2.out\"\n",
    "EN_FP = \"EN/\"\n",
    "FR_FP = \"FR/\"\n",
    "CN_FP = \"CN/\"\n",
    "SG_FP = \"SG/\"\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "SG has a '. . . . O' , must think of better way to split\n",
    "Need to fix stupid spacing in the words\n",
    "'''\n",
    "FPList = [EN_FP,FR_FP,CN_FP,SG_FP]\n",
    "\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    # print(modifiedData)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "    # print(huge_count_y_x)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    # # print(huge_emission)\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    # print(parsedtestData)\n",
    "    y_star = simple_sentiment_analysis(parsedtestData,modifiedData,huge_emission)\n",
    "    # print(y_star)\n",
    "    write_file(fp,\"dev.p2.out\",y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trash code\n",
    "'''global_y_x = {} #dictionary to keep the y and x\n",
    "    #place the data into the dictionary \n",
    "    for d in data:\n",
    "        x,y = d.split(\" \") #split by space \n",
    "        if x not in global_y_x:\n",
    "            global_y_x[x] = y '''\n",
    "def test_logic(test_data,modified_train_data):\n",
    "#     for w in test_data:\n",
    "#         if w not in modified_train_data:\n",
    "#             index = test_data.index(w)\n",
    "#             test_data[index] = \"#UNK#\"\n",
    "#         for test_words in modified_train_data:\n",
    "#             if test_words != w:\n",
    "#                 modified_train_data[modified_train_data.index(test_words)] = \"#UNK#\"\n",
    "#     for i in range(len(test_data)):\n",
    "#         for j in range(len(modified_train_data)):\n",
    "#             if test_data[i] != modified_train_data[j]:\n",
    "#                 print(test_data[i])\n",
    "#                 test_data[i] = \"#UNK#\"\n",
    "    td = {(\"hello\",4):1,(\"awegawega\",10):5}\n",
    "    bv = max(td, key=td.get)[1]\n",
    "    return bv\n",
    "\n",
    "modified_train_data =[\"i\",\"love\",\"you\"]\n",
    "test_data = [\"i\",\"hate\",\"you\",\"bobby\"]\n",
    "print(test_logic(test_data,modified_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. . . .', 'B-sadfsdf']\n"
     ]
    }
   ],
   "source": [
    "st =  '. . . . B-sadfsdf'\n",
    "print(st.rsplit(\" \",1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
