{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "y is the sequence of tags while x is the observed sequence of words \n",
    "Code dies when doing CN and SG files.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "from math import log \n",
    "#read the training data file \n",
    "def read_file_train(fp):\n",
    "    print(\"reading train file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "#read the test data file \n",
    "def read_file_test(fp):\n",
    "    print(\"reading tests file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "def write_file(fp,fn,data):\n",
    "    print(\"writing file...\")\n",
    "    with open(fp+fn, 'w',encoding='utf8') as f:\n",
    "        for k in data:\n",
    "            if k[0] == \"Blank\": #replace the blanks\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(k[0] + ' ' + k[1]+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    print(\"building count(y->x) & count(y)...\")\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    " \n",
    "    1. from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "    \n",
    "    returns huge_count_y_x and huge_count_y \n",
    "    huge_count_y_x = {(\"We\",\"B-Positive\"):1}\n",
    "    huge_count_y = {\"B-Positive\":1}\n",
    "    '''\n",
    "    \n",
    "    huge_count_y_x = {} #dict to contain all count(y->x)\n",
    "    huge_count_y = {} #dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x,y = d.split(\" \") #split by spaces \n",
    "        temp_tuple = (x,y)\n",
    "        \n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1 \n",
    "        \n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] =  1\n",
    "        else: \n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x,huge_count_y \n",
    "\n",
    "#part 2 first part \n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    print(\"calculating emission probability...\")\n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission ={} #dict to contain all the emission probabilities \n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        \n",
    "        emission = 0.0 #if none found, then return as 0 \n",
    "        tag = seq[1]\n",
    "#         print(k,v)\n",
    "        count_y = huge_count_y[tag] \n",
    "        emission = float(count_y_x / count_y)\n",
    "#         emission = float(log(count_y_x) - log(count_y +1))\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "# parsed_train = read_file_train(\"EN/train\")\n",
    "# print(calculate_emmision(parsed_train))\n",
    "\n",
    "#part 2 second part \n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "def modify_train_set(k,train_data):\n",
    "    print(\"modifying to #UNK#...\")\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "#         print(td)\n",
    "        word,tag = td.rsplit(\" \",1) \n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    #identify which words appeared less than k times \n",
    "    list_of_works_less_than_k = [] \n",
    "    for w,count in count_dict.items():\n",
    "        if count < k and w != \"Blank\": #Skip blanks. \n",
    "            list_of_works_less_than_k.append(w)\n",
    "    #now replace the entries in the training set  \n",
    "    for td in train_data:\n",
    "        word,tag = td.rsplit(\" \",1)\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "                \n",
    "#part 2 third part \n",
    "'''\n",
    "using the emission data, predict the possible tags (?) \n",
    "the tuple (x,y) is key to occurance of e(x|y) \n",
    "TODO:\n",
    "1. replace with #UNK# \n",
    "2. for each word w in the test_data\n",
    "    3. find the tuples in huge_emissison that matches w\n",
    "    4. if there's more than one, select the one with max(emission probability)\n",
    "    5. if there's only 1 result, then use that tag \n",
    "    6. append the tag to y_star\n",
    "7. write the results of w, tag into a tuple\n",
    "8. return the array y_star of tuples\n",
    "\n",
    "y_star = [(\"We\",\"O\"),(\"are\",\"I\"),...]\n",
    "'''\n",
    "def simple_sentiment_analysis(test_data,modified_train_data,huge_emission):\n",
    "    print(\"doing simple_sentiment_analysis...\")\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.rsplit(\" \",1)\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"Blank\": #Avoid blanks.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    y_star=[] #the tags, to be appended here \n",
    "    for w in test_data:\n",
    "        temp_dict = {} # to hold the possible results found from huge_emission \n",
    "        for key,e_value in huge_emission.items():\n",
    "            \n",
    "            if w == key[0]: \n",
    "                temp_dict[key] = e_value\n",
    "        #now return the tag that has the max e probability\n",
    "\n",
    "        best_result = max(temp_dict, key=temp_dict.get)[1]\n",
    "\n",
    "        #append a tupple for word, tag\n",
    "\n",
    "        y_star.append((w,best_result))\n",
    "    #return the dictionary -- don't do dic cos will remove duplicates \n",
    "\n",
    "    return y_star\n",
    "\n",
    "#part 3 first part \n",
    "'''\n",
    "first modify the train_data to get START and STOP in the data.\n",
    "train data = ['We A','are B','young C','Blank Blank','Hello D'] from read_file_train(fp+\"train\")\n",
    "test data = ['We','are','young','Blank','Hello'] from read_file_test(fp+\"dev.in\")\n",
    "Change to \n",
    "train data = ['START','We A','are B','young C','STOP','START','Hello D','STOP'] \n",
    "test data = ['START','We','are','young','STOP','START','Hello','STOP']\n",
    "'''\n",
    "def mod_data_for_transition(td,data_type):\n",
    "    print(\"Inserting START and STOP for \" + data_type +\"...\")\n",
    "    wtd = [] #words from train_data\n",
    "    if data_type == \"train\":\n",
    "        #extract the words first \n",
    "        for d in td:\n",
    "            w,t = d.rsplit(\" \",1) \n",
    "            wtd.append(t)\n",
    "    else:\n",
    "        wtd = td #don't need to extract \n",
    "    \n",
    "    #start replacing \n",
    "    for i in range(len(wtd)):\n",
    "        if wtd[i] == 'Blank':\n",
    "            index = i\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    wtd.insert(0,\"START\")\n",
    "    wtd[len(wtd)-1]=\"STOP\"\n",
    "    #TODO: am not sure why we need to do it twice. \n",
    "    for w in wtd:\n",
    "        if w == \"Blank\":\n",
    "            index = wtd.index(w)\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    return wtd \n",
    "    \n",
    "    \n",
    "    \n",
    "def build_dict_transition(td): \n",
    "    print(\"constructing dictionaries for transition...\")\n",
    "    '''\n",
    "    td=[\"START\",\"O\",\"O\",\"STOP\",\"START\",...]\n",
    "    1. build a tuple for (yi,yi-1)\n",
    "    2. put in dict \n",
    "    huge_count_yi_1_yi = {(yi,yi-1):1,...}\n",
    "    3. extract key[1] and put in huge_count_yi_1 \n",
    "    huge_count_yi_1 = {\"yi-1\":1,...}\n",
    "    '''\n",
    "    huge_count_yi_1_yi = {} #stores the no. of times of transition \n",
    "    huge_count_yi_1 = {} #stores no of times yi-1 occured \n",
    "#     print(len(td))\n",
    "    for i in range(1,len(td)):\n",
    "        # i = 0,1,2,...,n \n",
    "        temp = (td[i-1],td[i])\n",
    "        if temp not in huge_count_yi_1_yi:\n",
    "            huge_count_yi_1_yi[temp] = 1\n",
    "        else:\n",
    "            huge_count_yi_1_yi[temp] = huge_count_yi_1_yi[temp] + 1 \n",
    "#         print (temp)\n",
    "        if td[i-1] not in huge_count_yi_1:\n",
    "            huge_count_yi_1[td[i-1]] = 1\n",
    "        else:\n",
    "            huge_count_yi_1[td[i-1]] = huge_count_yi_1[td[i-1]] + 1\n",
    "            \n",
    "    return huge_count_yi_1_yi,huge_count_yi_1\n",
    "\n",
    "def calculate_transition(huge_count_yi_1_yi,huge_count_yi_1):\n",
    "    print(\"calculating transition probabilities...\")\n",
    "    '''\n",
    "    huge_transition = {(yi_1,yi):1,...}\n",
    "    '''\n",
    "    huge_transition = {}\n",
    "    for yi_1_yi,count_yi_1_yi in huge_count_yi_1_yi.items():\n",
    "        transition = 0.0 #ensure float\n",
    "        for yi_1,count_yi_1 in huge_count_yi_1.items(): \n",
    "            if yi_1 == yi_1_yi[0]:\n",
    "                transition = float(count_yi_1_yi / count_yi_1) #ensure float \n",
    "#                 transition = float(log(count_yi_1_yi) - log(count_yi_1))\n",
    "                huge_transition[yi_1_yi] = transition\n",
    "    return huge_transition\n",
    "\n",
    "#nightmare part 3 second part: viterbi algo. \n",
    "#td is test data \n",
    "def viterbi(emission,transition,td):  \n",
    "    print(\"Doing Viterbi...\")\n",
    "    massive_y_star = [] #contains tuples of word and tags \n",
    "    TAGS = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral'] #ALL TAGS IN. \n",
    "    #split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    temp = []\n",
    "#     all_pi = {}\n",
    "    for word in td:\n",
    "        if word != 'STOP':\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(\"STOP\")\n",
    "            seperated_sentence.append(temp)\n",
    "            temp =[]\n",
    "    #START OF VITERBI#\n",
    "    #try one sentence:\n",
    "#     fs = seperated_sentence[1]\n",
    "#     for base case y0 -> y1 \n",
    "    for fs in seperated_sentence:\n",
    "#         print(\"\\nDoing new sentence\")\n",
    "#         print(fs)\n",
    "        y_star = [] #contains tags \n",
    "#         print(\"Doing START\")\n",
    "        temp_pi={}\n",
    "        temp_all_tags_score = {}\n",
    "        all_pi = {}\n",
    "        for v in TAGS:\n",
    "            max_pi_k_v =0\n",
    "\n",
    "            pi_prev = 0.0 \n",
    "            isBased = check_basecase(fs[0])\n",
    "            if isBased :\n",
    "                pi_prev = 1.0\n",
    "            emission_value = 0.0\n",
    "            transition_value = 0.0\n",
    "            ekey = (fs[1],v)\n",
    "            tkey = ('START',v)\n",
    "            if ekey in emission:\n",
    "                emission_value = emission[(fs[1],v)]\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "#             print(\"em: \",emission_value,\"t: \",transition_value)\n",
    "            one_pi = pi_prev * emission_value * transition_value\n",
    "            temp_all_tags_score[v] = one_pi\n",
    "    #     print(temp_all_tags_score)    \n",
    "        all_pi[1] = temp_all_tags_score\n",
    "\n",
    "#         print(\"Doing forward recursive\")\n",
    "        for k in range(2,len(fs)-1): #ignore STOP\n",
    "#             print(\"\\nyi-1=\",fs[k-1],\"yi=\",fs[k])\n",
    "            temp_all_tags_score = {}\n",
    "            for v in TAGS:\n",
    "                max_pi_k_v = 0.0\n",
    "                temp_pi = {}\n",
    "\n",
    "                for u in TAGS:\n",
    "                    emission_current = 0.0 \n",
    "                    transition_current = 0.0 \n",
    "                    ekey = (fs[k],v)\n",
    "                    tkey = (u,v)\n",
    "    #                 print(ekey)\n",
    "    #                 print(tkey)\n",
    "                    if ekey in emission:\n",
    "                        emission_current = emission[ekey]\n",
    "                    if tkey in transition:\n",
    "                        transition_current = transition[tkey]\n",
    "                    #base case \n",
    "                    pi_prev = 0.0 \n",
    "                    if k-1 == 0: \n",
    "                        isBased = check_basecase(fs[k-1])\n",
    "                        if isBased:\n",
    "                            pi_prev = 1.0 \n",
    "                    else:\n",
    "                        pi_prev = all_pi[k-1][u]\n",
    "    #                 one_pi = pi_prev + emission_current + transition_current\n",
    "                    one_pi = pi_prev * emission_current * transition_current\n",
    "    #                 print(u,\"->\",v,\"Pi[k-1,v]: \",pi_prev,\" emission: \",emission_current,\" transition: \", transition_current,\" pi[k,v]: \", one_pi)\n",
    "                    temp_pi[u] = one_pi\n",
    "    #             print(\"\\nFINISHED COMPARING PREV NODES FOR CURRENT NODE:\",v,\" \",temp_pi)\n",
    "    #             print(\"\\ncapturing max score to node \",v , \"for \", fs[k]) \n",
    "                max_pi_k_v = max(temp_pi.values())\n",
    "    #             print(max_pi_k_v)\n",
    "                temp_all_tags_score[v] = max_pi_k_v\n",
    "    #             print(\"allocated:\", temp_all_tags_score)\n",
    "    #             print(\"\\nDoing next node.\")\n",
    "            all_pi[k] = temp_all_tags_score\n",
    "    #         print(\"\\n\")\n",
    "#         print(\"Doing STOP case\")\n",
    "        #for last case, yn -> yn+1 = STOP \n",
    "        k = len(fs) - 2\n",
    "        temp_pi_stop={}\n",
    "        temp_all_tags_score_stop = {}\n",
    "    #     print(\"Doing \",fs[k],\"->\",\"STOP\")\n",
    "#         print(all_pi)\n",
    "        for v in TAGS:\n",
    "            max_pi_k_v =0 \n",
    "            pi_prev = 0.0 \n",
    "            emission_value = 0.0\n",
    "            transition_value = 0.0\n",
    "            tkey = (v,'STOP') #fs[k+1] = STOP\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "#             print(len(fs),len(fs) - 2,k,k-1)\n",
    "            if len(fs) == 3:\n",
    "                pi_prev = all_pi[k][v]\n",
    "            else:\n",
    "                pi_prev = all_pi[k-1][v]\n",
    "            one_pi = pi_prev * transition_value\n",
    "#             print(\"t p: \",transition_value,\"pi[k-1,v]: \",pi_prev,\"pi[n+1,v]: \",one_pi)\n",
    "            temp_pi_stop[v] = one_pi \n",
    "    #         print(temp_pi_stop)\n",
    "        max_pi_k_v = max(temp_pi_stop.values())  \n",
    "        all_pi[len(fs)-1] = max_pi_k_v\n",
    "        \n",
    "#         print(\"Doing backwards STOP\")\n",
    "        #backwards \n",
    "        y_star.append('STOP')\n",
    "        #for STOP \n",
    "        n = len(fs) - 2 \n",
    "    #     pi_stop = all_pi[n]\n",
    "        yn_star_values= {}\n",
    "        for v in TAGS:\n",
    "            transition_value = 0.0\n",
    "            tkey = (v,'STOP') #fs[k+1] = STOP\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "            pi_current = all_pi[n][v] * transition_value\n",
    "            yn_star_values[v] = pi_current\n",
    "        yn_star = max(yn_star_values,key = yn_star_values.get)\n",
    "        y_star.insert(0,yn_star)\n",
    "\n",
    "#         print(\"Doing backwards\")\n",
    "        for n in range(len(fs)-3,1,-1): #don't include START \n",
    "            yn_1_star_values = {}\n",
    "#             print(\"\\ndoing word: \",fs[n])\n",
    "            for u in TAGS:\n",
    "                transition_value = 0.0\n",
    "                tkey = (u,y_star[0]) \n",
    "#                 print(tkey[0],\"->\",tkey[1])\n",
    "                if tkey in transition:\n",
    "                    transition_value = transition[tkey]\n",
    "    #             print(\"transS: \",transition_value)\n",
    "#                 print(\"pi[n-1,v]: \", all_pi[n][u])\n",
    "                pi_current = all_pi[n][u] * transition_value\n",
    "                yn_1_star_values[u] = pi_current\n",
    "#             print(\"all the values \",yn_1_star_values)\n",
    "            yn_1_star = max(yn_1_star_values, key = yn_1_star_values.get)\n",
    "    #         print(yn_1_star)\n",
    "            y_star.insert(0,yn_1_star)\n",
    "\n",
    "        #do START -> y1 \n",
    "#         print(\"Doing backwards START\")\n",
    "        n = 1 \n",
    "        yn_star_values_start= {}\n",
    "        for v in TAGS:\n",
    "#             print(\"\\ndoing START->\",v,\"for word \",fs[n])\n",
    "            transition_value = 0.0\n",
    "            tkey = ('START',v)\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "            pi_current = all_pi[n][v] * transition_value\n",
    "            yn_star_values_start[v] = pi_current\n",
    "        yn_star_start = max(yn_star_values_start, key = yn_star_values_start.get)\n",
    "        y_star.insert(0,yn_star_start)\n",
    "#         print(\"\\n\")\n",
    "#         print(all_pi)\n",
    "#         print(\"\\n\")\n",
    "        y_star.insert(0,\"START\")\n",
    "#         print(fs)\n",
    "#         print(y_star)\n",
    "        massive_y_star.append(y_star)\n",
    "#     print(massive_y_star)\n",
    "    massive_y_star = process_viterbi_results(massive_y_star,seperated_sentence)\n",
    "    return massive_y_star\n",
    "\n",
    "def check_basecase(state):\n",
    "    if state=='START':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def mod_testdata(test_data,modified_train_data):\n",
    "    print(\"Replacing to #UNK# for test...\")\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.rsplit(\" \",1)\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"START\" and w!= \"STOP\": #Avoid START and STOP.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    return test_data\n",
    "\n",
    "#returns in (word,tag), with START STOP repleaced, Blank returns. \n",
    "def process_viterbi_results(massive_y_star,seperated_sentence):\n",
    "    tagged_words = [] #return this \n",
    "    for i in range(len(massive_y_star)):\n",
    "        for j in range(len(massive_y_star[i])):\n",
    "            if massive_y_star[i][j] == \"START\":\n",
    "                continue #don't do anything \n",
    "            elif massive_y_star[i][j] == \"STOP\":\n",
    "                #replace to blank blank \n",
    "                temp = ('Blank','Blank')\n",
    "                tagged_words.append(temp)\n",
    "            else: #normal words \n",
    "                temp = (seperated_sentence[i][j],massive_y_star[i][j])\n",
    "                tagged_words.append(temp)\n",
    "    return tagged_words\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TAGS = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral'] #ALL TAGS IN. \n",
    "\n",
    "#applicable to all cases \n",
    "def alpha_base(transition):\n",
    "    alpha_base = {}\n",
    "    for u in TAGS:\n",
    "        if ('START',u) in transition:\n",
    "            alpha_base[u] = transition[('START',u)]\n",
    "        else:\n",
    "            alpha_base[u] = 0.0\n",
    "    return alpha_base\n",
    "\n",
    "#tag u from Y node_no , where word is from Xn from the list of words \n",
    "def beta_base(transition,emission,sentence):\n",
    "    X_n = sentence[len(sentence)-2] #last word \n",
    "    beta_base = {} \n",
    "    for u in TAGS:\n",
    "        transition_v = 0.0 \n",
    "        emission_v = 0.0 \n",
    "        if (u,'STOP') in transition:\n",
    "            transition_v = transition[(u,'STOP')]\n",
    "        if (X_n,u) in emission:\n",
    "            emission_v = emission[(X_n,u)]\n",
    "        beta_base[u] = float(transition_v * emission_v)\n",
    "    return beta_base\n",
    "\n",
    "\n",
    "def split_sentence(td):\n",
    "    print(\"Splitting sentence...\") \n",
    "    #split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    temp = []\n",
    "    for word in td:\n",
    "#         print(word == \"STOP\")\n",
    "        if word != \"STOP\":\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(\"STOP\")\n",
    "#             print(temp)\n",
    "            seperated_sentence.append(temp)\n",
    "            temp =[]\n",
    "    return seperated_sentence\n",
    "\n",
    "def max_marginal(td,transition,emission,alpha_base):\n",
    "    sentences = split_sentence(td)\n",
    "    sentence = sentences[0] #test on one sentence first \n",
    "    print(sentence)\n",
    "    betaBase = beta_base(transition,emission,sentence) #get the beta base \n",
    "    forward = {} \n",
    "    forward[1] = alpha_base #this should be constant\n",
    "    YSTAR = [] \n",
    "    #now do forward \n",
    "    for j in range(1,len(sentence)-1):\n",
    "#         print(sentence[j])\n",
    "        temp_alphas = {}\n",
    "        for u in TAGS:\n",
    "            alpha_j_1_v = 0.0 #value of alpha u (j+1) \n",
    "            for v in TAGS:\n",
    "                a = 0.0 \n",
    "                b = 0.0 \n",
    "                if (v,u) in transition:\n",
    "                    a = transition[(v,u)]\n",
    "                if (sentence[j],v) in emission:\n",
    "                    b = emission[(sentence[j],v)]\n",
    "                alpha_j_1_v += float(forward[j][v] * a * b )\n",
    "            temp_alphas[u] = alpha_j_1_v\n",
    "        forward[j+1]= temp_alphas\n",
    "#     return forward\n",
    "    #now do backward\n",
    "    backwards = {}\n",
    "    backwards[len(sentence)-1] = betaBase # inserting beta u n\n",
    "    for j in range(len(sentence)-2,0,-1): #j = n-1 ,...,1 \n",
    "        temp_beta={}\n",
    "        for u in TAGS: \n",
    "            beta_u_j = 0.0 \n",
    "            for v in TAGS:\n",
    "                a = 0.0 \n",
    "                b = 0.0 \n",
    "                if (u,v) in transition:\n",
    "                    a = transition[(u,v)]\n",
    "                if (sentence[j],u) in emission:\n",
    "                    b = emission[(sentence[j],u)]\n",
    "                beta_u_j += float(backwards[j+1][v]*a*b)\n",
    "            temp_beta[u] = beta_u_j\n",
    "        backwards[j] = temp_beta\n",
    "\n",
    "#     return backwards\n",
    "    #find the tags\n",
    "    for j in range(1,len(sentence)-1):\n",
    "\n",
    "#         YSTAR.append(max(forward[j],key=forward.get) * max(backwards[j].values(),key=backwards.get))\n",
    "        temp = {}\n",
    "        for u in TAGS:\n",
    "            temp[u] = forward[j][u] * backwards[j][u]\n",
    "        YSTAR.append(max(temp,key=temp.get))\n",
    "    return YSTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to run the file \n",
    "PART2_FN = \"dev.p2.out\"\n",
    "PART3_FN = \"dev.p3.out\"\n",
    "PART4_FN = \"dev.p4.out\"\n",
    "EN_FP = \"EN/\"\n",
    "FR_FP = \"FR/\"\n",
    "CN_FP = \"CN/\"\n",
    "SG_FP = \"SG/\"\n",
    "# FPList = [EN_FP,FR_FP,CN_FP,SG_FP]\n",
    "FPList = [EN_FP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Splitting sentence...\n",
      "['START', 'When', 'I', 'called', 'this', '#UNK#', ',', 'I', \"didn't\", 'think', 'I', 'would', 'be', 'able', 'to', 'get', 'in', 'at', '#UNK#', ',', 'but', 'I', 'was', 'able', 'to', 'get', 'in', ',', '#UNK#', 'with', 'four', 'other', 'guests', '.', 'STOP']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "End...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#part 4\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    stsp_train = (mod_data_for_transition(modifiedData,\"train\"))\n",
    "#     print(stsp_train)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    huge_count_yi_1_yi,huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "#     print(huge_count_yi_1)\n",
    "    huge_transition = calculate_transition(huge_count_yi_1_yi,huge_count_yi_1)\n",
    "#     print(huge_transition)\n",
    "#     td = huge_transition\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    stsp_test = mod_data_for_transition(parsedtestData,\"test\")\n",
    "    modifiedTestdata = mod_testdata(stsp_test,modifiedData)\n",
    "#     transition,emission,sentence\n",
    "#     print(split_sentence(modifiedTestdata)[0])\n",
    "#     print(beta_base(huge_transition,huge_emission,split_sentence(modifiedTestdata)[0]))\n",
    "    print(max_marginal(modifiedTestdata,huge_transition,huge_emission,alpha_base(huge_transition)))\n",
    "    print(\"End...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 3 \n",
    "print(\"Part 3 running...\")\n",
    "td ={}\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    stsp_train = (mod_data_for_transition(modifiedData,\"train\"))\n",
    "#     print(stsp_train)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    \n",
    "    huge_count_yi_1_yi,huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "#     print(huge_count_yi_1)\n",
    "    huge_transition = calculate_transition(huge_count_yi_1_yi,huge_count_yi_1)\n",
    "#     print(huge_transition)\n",
    "    td = huge_transition\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    \n",
    "    stsp_test = mod_data_for_transition(parsedtestData,\"test\")\n",
    "    modifiedTestdata = mod_testdata(stsp_test,modifiedData)\n",
    "#     print(modifiedTestdata)\n",
    "    massive_y_star =viterbi(huge_emission,huge_transition,modifiedTestdata)\n",
    "    print(massive_y_star)\n",
    "#     write_file(fp,PART3_FN,massive_y_star)\n",
    "    print(\"End...\\n\")\n",
    "\n",
    "# for k,v in td.items():\n",
    "#     print(k,v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 running...\n",
      "\n",
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "building count(y->x) & count(y)...\n",
      "\n",
      "calculating emission probability...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "doing simple_sentiment_analysis...\n",
      "\n",
      "writing file...\n",
      "\n",
      "End...\n",
      "('My', 'O') 0.0014437752660671562\n"
     ]
    }
   ],
   "source": [
    "#part 2\n",
    "print(\"Part 2 running...\\n\")\n",
    "he = {}\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    # print(modifiedData)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "#     print(huge_count_y)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    he = huge_emission\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    # print(parsedtestData)\n",
    "    y_star = simple_sentiment_analysis(parsedtestData,modifiedData,huge_emission)\n",
    "    # print(y_star)\n",
    "    write_file(fp,PART2_FN,y_star)\n",
    "    print(\"End...\")\n",
    "\n",
    "# print(he)\n",
    "# for k,v in he.items():\n",
    "#     if k[0] == 'My':\n",
    "#         print(k,v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
