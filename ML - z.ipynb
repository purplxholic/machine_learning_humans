{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "y is the sequence of tags while x is the observed sequence of words \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "#read the training data file \n",
    "def read_file_train(fp):\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n',' '))\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "def write_file(fp,fn,data):\n",
    "    with open(fp+fn, 'w',encoding='utf8') as f:\n",
    "        for k in data:\n",
    "            print(\"data received:\")\n",
    "            print(k[0] + ' ' + k[1]+'\\n')\n",
    "            f.write(k[0] + ' ' + k[1]+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    "    ''' \n",
    "    '''\n",
    "    1.  from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "    '''\n",
    "    huge_count_y_x = {} #dict to contain all count(y->x)\n",
    "    huge_count_y = {} #dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x,y = d.split(\" \") #split by spaces \n",
    "        temp_tuple = (x,y)\n",
    "        \n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1 \n",
    "        \n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] =  1\n",
    "        else: \n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x,huge_count_y \n",
    "\n",
    "#part 2 first part \n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    \n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission ={} #dict to contain all the emission probabilities \n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        \n",
    "        emission = 0.0 #if none found, then return as 0 \n",
    "        tag = seq[1]\n",
    "#         print(k,v)\n",
    "        count_y = huge_count_y[tag] \n",
    "        emission = count_y_x / count_y\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "# parsed_train = read_file_train(\"EN/train\")\n",
    "# print(calculate_emmision(parsed_train))\n",
    "\n",
    "#part 2 second part \n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "def modify_train_set(k,train_data):\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "#         print(td)\n",
    "        word,tag = td.split(\" \") \n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    #identify which words appeared less than k times \n",
    "    list_of_works_less_than_k = [] \n",
    "    for w,count in count_dict.items():\n",
    "        if count < k and w != ' ': #TODO: FIX THE SPACE MESS\n",
    "            list_of_works_less_than_k.append(w)\n",
    "    #now replace the entries in the training set  \n",
    "    for td in train_data:\n",
    "        word,tag = td.split(\" \")\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "                \n",
    "#part 2 third part \n",
    "'''\n",
    "using the emission data, predict the possible tags (?) \n",
    "the tuple (x,y) is key to occurance of e(x|y) \n",
    "TODO:\n",
    "1. replace with #UNK# \n",
    "2. for each word w in the test_data\n",
    "    3. find the tuples in huge_emissison that matches w\n",
    "    4. if there's more than one, select the one with max(emission probability)\n",
    "    5. if there's only 1 result, then use that tag \n",
    "    6. append the tag to y_star\n",
    "7. write the results of w, tag into a tuple\n",
    "8. return the array of tuples\n",
    "'''\n",
    "def simple_sentiment_analysis(test_data,modified_train_data,huge_emission):\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.split(\" \")\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \" \":\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    y_star=[] #the tags, to be appended here \n",
    "#     print(\"test data:\\n\" , len(test_data))\n",
    "    for w in test_data:\n",
    "        temp_dict = {} # to hold the possible results found from huge_emission \n",
    "        for key,e_value in huge_emission.items():\n",
    "            if w == key[0] and w != \" \": #TODO: FIX THE SPACE MESS if it matches the word\n",
    "                temp_dict[key] = e_value\n",
    "        #now return the tag that has the max e probability \n",
    "        best_result = max(temp_dict, key=temp_dict.get)[1]\n",
    "        #append a tupple for word, tag\n",
    "        y_star.append((w,best_result))\n",
    "    #return the dictionary -- don't do dic cos will remove duplicates \n",
    "#     predicted_data = dict(zip(test_data, y_star))\n",
    "#     print(predicted_data)\n",
    "    return y_star\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-8efc45d5df02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mparsedtestData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_file_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEN_FP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"dev.in\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# print(parsedtestData)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0my_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_sentiment_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsedtestData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodifiedData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhuge_emission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# print(y_star)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mwrite_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEN_FP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"dev.p2.out\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_star\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-191-18a22308e6e8>\u001b[0m in \u001b[0;36msimple_sentiment_analysis\u001b[1;34m(test_data, modified_train_data, huge_emission)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mtemp_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;31m#now return the tag that has the max e probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mbest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;31m#append a tupple for word, tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0my_star\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "#code to run the file \n",
    "PART2_FN = \"dev.p2.out\"\n",
    "EN_FP = \"EN/\"\n",
    "FR_FP = \"FR/\"\n",
    "CN_FP = \"CN/\"\n",
    "SG_FP = \"SG/\"\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "SG has a '. . . . O' , must think of better way to split\n",
    "Need to fix stupid spacing in the words\n",
    "'''\n",
    "\n",
    "parsedtrainData = read_file_train(EN_FP+\"train\")\n",
    "# print(parsedtrainData)\n",
    "modifiedData = modify_train_set(3,parsedtrainData)\n",
    "\n",
    "huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "# print(huge_count_y_x)\n",
    "huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "# print(huge_emission)\n",
    "parsedtestData = read_file_train(EN_FP+\"dev.in\")\n",
    "# print(parsedtestData)\n",
    "y_star = simple_sentiment_analysis(parsedtestData,modifiedData,huge_emission)\n",
    "# print(y_star)\n",
    "write_file(EN_FP,\"dev.p2.out\",y_star)\n",
    "\n",
    "\n",
    "\n",
    "# FPList = [EN_FP,FR_FP,CN_FP,SG_FP]\n",
    "\n",
    "# for i in FPList:\n",
    "#     parsedtrainData = read_file_train(i +\"train\")\n",
    "\n",
    "#     modifiedData = modify_train_set(3,parsedtrainData)\n",
    "\n",
    "#     huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "#     # print(huge_count_y_x)\n",
    "#     huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "#     # print(huge_emission)\n",
    "#     parsedtestData = read_file_train(i + \"dev.in\")\n",
    "#     # print(parsedtestData)\n",
    "#     y_star = simple_sentiment_analysis(parsedtestData,modifiedData,huge_emission)\n",
    "#     write_file(i,\"dev.p2.out\",y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trash code\n",
    "'''global_y_x = {} #dictionary to keep the y and x\n",
    "    #place the data into the dictionary \n",
    "    for d in data:\n",
    "        x,y = d.split(\" \") #split by space \n",
    "        if x not in global_y_x:\n",
    "            global_y_x[x] = y '''\n",
    "def test_logic(test_data,modified_train_data):\n",
    "#     for w in test_data:\n",
    "#         if w not in modified_train_data:\n",
    "#             index = test_data.index(w)\n",
    "#             test_data[index] = \"#UNK#\"\n",
    "#         for test_words in modified_train_data:\n",
    "#             if test_words != w:\n",
    "#                 modified_train_data[modified_train_data.index(test_words)] = \"#UNK#\"\n",
    "#     for i in range(len(test_data)):\n",
    "#         for j in range(len(modified_train_data)):\n",
    "#             if test_data[i] != modified_train_data[j]:\n",
    "#                 print(test_data[i])\n",
    "#                 test_data[i] = \"#UNK#\"\n",
    "    td = {(\"hello\",4):1,(\"awegawega\",10):5}\n",
    "    bv = max(td, key=td.get)[1]\n",
    "    return bv\n",
    "\n",
    "modified_train_data =[\"i\",\"love\",\"you\"]\n",
    "test_data = [\"i\",\"hate\",\"you\",\"bobby\"]\n",
    "print(test_logic(test_data,modified_train_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
