{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "y is the sequence of tags while x is the observed sequence of words \n",
    "Code dies when doing CN and SG files.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "#read the training data file \n",
    "def read_file_train(fp):\n",
    "    print(\"reading train file...\\n\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "#read the test data file \n",
    "def read_file_test(fp):\n",
    "    print(\"reading tests file...\\n\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "def write_file(fp,fn,data):\n",
    "    print(\"writing file...\\n\")\n",
    "    with open(fp+fn, 'w',encoding='utf8') as f:\n",
    "        for k in data:\n",
    "#             print(\"data received:\")\n",
    "#             print(k[0] + ' ' + k[1]+'\\n')\n",
    "            if k[0] == \"Blank\": #replace the blanks\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(k[0] + ' ' + k[1]+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    print(\"building count(y->x) & count(y)...\\n\")\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    " \n",
    "    1. from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "    \n",
    "    returns huge_count_y_x and huge_count_y \n",
    "    huge_count_y_x = {(\"We\",\"B-Positive\"):1}\n",
    "    huge_count_y = {\"B-Positive\":1}\n",
    "    '''\n",
    "    \n",
    "    huge_count_y_x = {} #dict to contain all count(y->x)\n",
    "    huge_count_y = {} #dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x,y = d.split(\" \") #split by spaces \n",
    "        temp_tuple = (x,y)\n",
    "        \n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1 \n",
    "        \n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] =  1\n",
    "        else: \n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x,huge_count_y \n",
    "\n",
    "#part 2 first part \n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    print(\"calculating emission probability...\\n\")\n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission ={} #dict to contain all the emission probabilities \n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        \n",
    "        emission = 0.0 #if none found, then return as 0 \n",
    "        tag = seq[1]\n",
    "#         print(k,v)\n",
    "        count_y = huge_count_y[tag] \n",
    "        emission = count_y_x / count_y\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "# parsed_train = read_file_train(\"EN/train\")\n",
    "# print(calculate_emmision(parsed_train))\n",
    "\n",
    "#part 2 second part \n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "def modify_train_set(k,train_data):\n",
    "    print(\"modifying to #UNK#...\\n\")\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "#         print(td)\n",
    "        word,tag = td.rsplit(\" \",1) \n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    #identify which words appeared less than k times \n",
    "    list_of_works_less_than_k = [] \n",
    "    for w,count in count_dict.items():\n",
    "        if count < k and w != \"Blank\": #Skip blanks. \n",
    "            list_of_works_less_than_k.append(w)\n",
    "    #now replace the entries in the training set  \n",
    "    for td in train_data:\n",
    "        word,tag = td.rsplit(\" \",1)\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "                \n",
    "#part 2 third part \n",
    "'''\n",
    "using the emission data, predict the possible tags (?) \n",
    "the tuple (x,y) is key to occurance of e(x|y) \n",
    "TODO:\n",
    "1. replace with #UNK# \n",
    "2. for each word w in the test_data\n",
    "    3. find the tuples in huge_emissison that matches w\n",
    "    4. if there's more than one, select the one with max(emission probability)\n",
    "    5. if there's only 1 result, then use that tag \n",
    "    6. append the tag to y_star\n",
    "7. write the results of w, tag into a tuple\n",
    "8. return the array y_star of tuples\n",
    "\n",
    "y_star = [(\"We\",\"O\"),(\"are\",\"I\"),...]\n",
    "'''\n",
    "def simple_sentiment_analysis(test_data,modified_train_data,huge_emission):\n",
    "    print(\"doing simple_sentiment_analysis...\\n\")\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.rsplit(\" \",1)\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"Blank\": #Avoid blanks.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    y_star=[] #the tags, to be appended here \n",
    "    for w in test_data:\n",
    "        temp_dict = {} # to hold the possible results found from huge_emission \n",
    "        for key,e_value in huge_emission.items():\n",
    "            #TODO: FIX THE SPACE MESS if it matches the word\n",
    "            if w == key[0]: \n",
    "                temp_dict[key] = e_value\n",
    "        #now return the tag that has the max e probability\n",
    "\n",
    "        best_result = max(temp_dict, key=temp_dict.get)[1]\n",
    "\n",
    "        #append a tupple for word, tag\n",
    "\n",
    "        y_star.append((w,best_result))\n",
    "    #return the dictionary -- don't do dic cos will remove duplicates \n",
    "\n",
    "    return y_star\n",
    "\n",
    "#part 3 first part \n",
    "'''\n",
    "first modify the train_data to get START and STOP in the data.\n",
    "train data = ['We A','are B','young C','Blank Blank','Hello D'] from read_file_train(fp+\"train\")\n",
    "test data = ['We','are','young','Blank','Hello'] from read_file_test(fp+\"dev.in\")\n",
    "Change to \n",
    "train data = ['START','We A','are B','young C','STOP','START','Hello D','STOP'] \n",
    "test data = ['START','We','are','young','STOP','START','Hello','STOP']\n",
    "'''\n",
    "def mod_data_for_transition(td,data_type):\n",
    "    print(\"Inserting START and STOP for \" + data_type +\"...\\n\")\n",
    "    wtd = [] #words from train_data\n",
    "    if data_type == \"train\":\n",
    "        #extract the words first \n",
    "        for d in td:\n",
    "            w,t = d.rsplit(\" \",1) \n",
    "            wtd.append(w)\n",
    "    else:\n",
    "        wtd = td #don't need to extract \n",
    "    \n",
    "    #start replacing \n",
    "    for i in range(len(wtd)):\n",
    "        if wtd[i] == 'Blank':\n",
    "            index = i\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    wtd.insert(0,\"START\")\n",
    "    wtd[len(wtd)-1]=\"STOP\"\n",
    "    #TODO: am not sure why we need to do it twice. \n",
    "    for w in wtd:\n",
    "        if w == \"Blank\":\n",
    "            index = wtd.index(w)\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    return wtd \n",
    "    \n",
    "    \n",
    "    \n",
    "def build_dict_transition(td): \n",
    "    print(\"constructing dictionaries for transition...\\n\")\n",
    "    '''\n",
    "    td=[strings]\n",
    "    1. build a tuple for (yi,yi-1)\n",
    "    2. put in dict \n",
    "    huge_count_yi_1_yi = {(yi,yi-1):1,...}\n",
    "    3. extract key[1] and put in huge_count_yi_1 \n",
    "    huge_count_yi_1 = {\"yi-1\":1,...}\n",
    "    '''\n",
    "    huge_count_yi_1_yi = {} #stores the no. of times of transition \n",
    "    huge_count_yi_1 = {} #stores no of times yi-1 occured \n",
    "#     print(len(td))\n",
    "    for i in range(1,len(td)):\n",
    "        # i = 0,1,2,...,n \n",
    "        temp = (td[i-1],td[i])\n",
    "        if temp not in huge_count_yi_1_yi:\n",
    "            huge_count_yi_1_yi[temp] = 1\n",
    "        else:\n",
    "            huge_count_yi_1_yi[temp] = huge_count_yi_1_yi[temp] + 1 \n",
    "#         print (temp)\n",
    "        if td[i-1] not in huge_count_yi_1:\n",
    "            huge_count_yi_1[td[i-1]] = 1\n",
    "        else:\n",
    "            huge_count_yi_1[td[i-1]] = huge_count_yi_1[td[i-1]] + 1\n",
    "            \n",
    "    return huge_count_yi_1_yi,huge_count_yi_1\n",
    "\n",
    "def calculate_transition(huge_count_yi_1_yi,huge_count_yi_1):\n",
    "    print(\"calculating transition probabilities...\\n\")\n",
    "    '''\n",
    "    huge_transition = {(yi_1,yi):1,...}\n",
    "    '''\n",
    "    huge_transition = {}\n",
    "    for yi_1_yi,count_yi_1_yi in huge_count_yi_1_yi.items():\n",
    "        transition = 0.0 #ensure float\n",
    "        for yi_1,count_yi_1 in huge_count_yi_1.items(): \n",
    "            if yi_1 == yi_1_yi[0]:\n",
    "                transition = float(count_yi_1_yi / count_yi_1) #ensure float \n",
    "                huge_transition[yi_1_yi] = transition\n",
    "    return huge_transition\n",
    "\n",
    "#nightmare part 3 second part: viterbi algo. \n",
    "def viterbi(emissison,transition):\n",
    "    y_star = [] #contains tuples of word and tags \n",
    "    return y_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to run the file \n",
    "PART2_FN = \"dev.p2.out\"\n",
    "PART3_FN = \"dev.p3.out\"\n",
    "PART4_FN = \"dev.p4.out\"\n",
    "EN_FP = \"EN/\"\n",
    "FR_FP = \"FR/\"\n",
    "CN_FP = \"CN/\"\n",
    "SG_FP = \"SG/\"\n",
    "FPList = [EN_FP,FR_FP,CN_FP,SG_FP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "Inserting START and STOP for train...\n",
      "\n",
      "constructing dictionaries for transition...\n",
      "\n",
      "calculating transition probabilities...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "Inserting START and STOP for test...\n",
      "\n",
      "\n",
      "Doing FR/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "Inserting START and STOP for train...\n",
      "\n",
      "constructing dictionaries for transition...\n",
      "\n",
      "calculating transition probabilities...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "Inserting START and STOP for test...\n",
      "\n",
      "\n",
      "Doing CN/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "Inserting START and STOP for train...\n",
      "\n",
      "constructing dictionaries for transition...\n",
      "\n",
      "calculating transition probabilities...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "Inserting START and STOP for test...\n",
      "\n",
      "\n",
      "Doing SG/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "Inserting START and STOP for train...\n",
      "\n",
      "constructing dictionaries for transition...\n",
      "\n",
      "calculating transition probabilities...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "Inserting START and STOP for test...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#part 3 \n",
    "print(\"Part 3 running...\\n\")\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    stsp_train = (mod_data_for_transition(modifiedData,\"train\"))\n",
    "    huge_count_yi_1_yi,huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "    huge_transition = calculate_transition(huge_count_yi_1_yi,huge_count_yi_1)\n",
    "    # print(huge_transition)\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    stsp_test = mod_data_for_transition(parsedtestData,\"test\")\n",
    "    print(\"End...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "building count(y->x) & count(y)...\n",
      "\n",
      "calculating emission probability...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "doing simple_sentiment_analysis...\n",
      "\n",
      "writing file...\n",
      "\n",
      "End...\n",
      "\n",
      "Doing FR/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "building count(y->x) & count(y)...\n",
      "\n",
      "calculating emission probability...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "doing simple_sentiment_analysis...\n",
      "\n",
      "writing file...\n",
      "\n",
      "End...\n",
      "\n",
      "Doing CN/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "building count(y->x) & count(y)...\n",
      "\n",
      "calculating emission probability...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "doing simple_sentiment_analysis...\n",
      "\n",
      "writing file...\n",
      "\n",
      "End...\n",
      "\n",
      "Doing SG/...\n",
      "reading train file...\n",
      "\n",
      "modifying to #UNK#...\n",
      "\n",
      "building count(y->x) & count(y)...\n",
      "\n",
      "calculating emission probability...\n",
      "\n",
      "reading tests file...\n",
      "\n",
      "doing simple_sentiment_analysis...\n",
      "\n",
      "writing file...\n",
      "\n",
      "End...\n"
     ]
    }
   ],
   "source": [
    "#part 2\n",
    "print(\"Part 2 running...\\n\")\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    # print(modifiedData)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "    # print(huge_count_y_x)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    # # print(huge_emission)\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    # print(parsedtestData)\n",
    "    y_star = simple_sentiment_analysis(parsedtestData,modifiedData,huge_emission)\n",
    "    # print(y_star)\n",
    "    write_file(fp,PART2_FN,y_star)\n",
    "    print(\"End...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
