{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "y is the sequence of tags while x is the observed sequence of words \n",
    "Code dies when doing CN and SG files.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to run the file \n",
    "PART2_FN = \"dev.p2.out\"\n",
    "PART3_FN = \"dev.p3.out\"\n",
    "PART4_FN = \"dev.p4.out\"\n",
    "EN_FP = \"EN/\"\n",
    "FR_FP = \"FR/\"\n",
    "CN_FP = \"CN/\"\n",
    "SG_FP = \"SG/\"\n",
    "# FPList = [EN_FP,FR_FP,CN_FP,SG_FP]\n",
    "FPList = [EN_FP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "from math import log \n",
    "#read the training data file \n",
    "def read_file_train(fp):\n",
    "    print(\"reading train file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "#read the test data file \n",
    "def read_file_test(fp):\n",
    "    print(\"reading tests file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "def write_file(fp,fn,data):\n",
    "    print(\"writing file...\")\n",
    "    with open(fp+fn, 'w',encoding='utf8') as f:\n",
    "        for k in data:\n",
    "            if k[0] == \"Blank\": #replace the blanks\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(k[0] + ' ' + k[1]+'\\n')\n",
    "    f.close()\n",
    "\n",
    "#returns in (word,tag), with START STOP replaced, Blank returns. \n",
    "def process_results(massive_y_star,seperated_sentence):\n",
    "    tagged_words = [] #return this \n",
    "    for i in range(len(massive_y_star)):\n",
    "        for j in range(len(massive_y_star[i])):\n",
    "            if massive_y_star[i][j] == \"START\":\n",
    "                continue #don't do anything \n",
    "            elif massive_y_star[i][j] == \"STOP\":\n",
    "                #replace to blank blank \n",
    "                temp = ('Blank','Blank')\n",
    "                tagged_words.append(temp)\n",
    "            else: #normal words \n",
    "                temp = (seperated_sentence[i][j],massive_y_star[i][j])\n",
    "                tagged_words.append(temp)\n",
    "    return tagged_words\n",
    "\n",
    "def split_sentence(td):\n",
    "    print(\"Splitting sentence...\") \n",
    "    #split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    temp = []\n",
    "    for word in td:\n",
    "        if word != \"STOP\":\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(\"STOP\")\n",
    "            seperated_sentence.append(temp)\n",
    "            temp =[]\n",
    "    return seperated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    print(\"building count(y->x) & count(y)...\")\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    " \n",
    "    1. from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "    \n",
    "    returns huge_count_y_x and huge_count_y \n",
    "    huge_count_y_x = {(\"We\",\"B-Positive\"):1}\n",
    "    huge_count_y = {\"B-Positive\":1}\n",
    "    '''\n",
    "    \n",
    "    huge_count_y_x = {} #dict to contain all count(y->x)\n",
    "    huge_count_y = {} #dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x,y = d.split(\" \") #split by spaces \n",
    "        temp_tuple = (x,y)\n",
    "        \n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1 \n",
    "        \n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] =  1\n",
    "        else: \n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x,huge_count_y \n",
    "\n",
    "#part 2 first part \n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    print(\"calculating emission probability...\")\n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission ={} #dict to contain all the emission probabilities \n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        \n",
    "        emission = 0.0 #if none found, then return as 0 \n",
    "        tag = seq[1]\n",
    "#         print(k,v)\n",
    "        count_y = huge_count_y[tag] \n",
    "        emission = float(count_y_x / count_y)\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "#part 2 second part \n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "def modify_train_set(k,train_data):\n",
    "    print(\"modifying to #UNK#...\")\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "#         print(td)\n",
    "        word,tag = td.rsplit(\" \",1) \n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    #identify which words appeared less than k times \n",
    "    list_of_works_less_than_k = [] \n",
    "    for w,count in count_dict.items():\n",
    "        if count < k and w != \"Blank\": #Skip blanks. \n",
    "            list_of_works_less_than_k.append(w)\n",
    "    #now replace the entries in the training set  \n",
    "    for td in train_data:\n",
    "        word,tag = td.rsplit(\" \",1)\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "                \n",
    "#part 2 third part \n",
    "'''\n",
    "using the emission data, predict the possible tags (?) \n",
    "the tuple (x,y) is key to occurance of e(x|y) \n",
    "TODO:\n",
    "1. replace with #UNK# \n",
    "2. for each word w in the test_data\n",
    "    3. find the tuples in huge_emissison that matches w\n",
    "    4. if there's more than one, select the one with max(emission probability)\n",
    "    5. if there's only 1 result, then use that tag \n",
    "    6. append the tag to y_star\n",
    "7. write the results of w, tag into a tuple\n",
    "8. return the array y_star of tuples\n",
    "\n",
    "y_star = [(\"We\",\"O\"),(\"are\",\"I\"),...]\n",
    "'''\n",
    "def simple_sentiment_analysis(test_data,modified_train_data,huge_emission):\n",
    "    print(\"doing simple_sentiment_analysis...\")\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.rsplit(\" \",1)\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"Blank\": #Avoid blanks.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    y_star=[] #the tags, to be appended here \n",
    "    for w in test_data:\n",
    "        temp_dict = {} # to hold the possible results found from huge_emission \n",
    "        for key,e_value in huge_emission.items():\n",
    "            \n",
    "            if w == key[0]: \n",
    "                temp_dict[key] = e_value\n",
    "        #now return the tag that has the max e probability\n",
    "\n",
    "        best_result = max(temp_dict, key=temp_dict.get)[1]\n",
    "\n",
    "        #append a tupple for word, tag\n",
    "\n",
    "        y_star.append((w,best_result))\n",
    "    return y_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 running...\n",
      "\n",
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "reading tests file...\n",
      "doing simple_sentiment_analysis...\n",
      "writing file...\n",
      "End...\n"
     ]
    }
   ],
   "source": [
    "#part 2\n",
    "print(\"Part 2 running...\\n\")\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    # print(modifiedData)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "#     print(huge_count_y)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    he = huge_emission\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    # print(parsedtestData)\n",
    "    y_star = simple_sentiment_analysis(parsedtestData,modifiedData,huge_emission)\n",
    "    # print(y_star)\n",
    "    write_file(fp,PART2_FN,y_star)\n",
    "    print(\"End...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report \n",
    "Using the evalResult.py script provided, \n",
    "\n",
    "#### EN\n",
    "\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 1201\n",
    "\n",
    "#Correct Entity : 165\n",
    "Entity  precision: 0.1374\n",
    "Entity  recall: 0.7301\n",
    "Entity  F: 0.2313\n",
    "\n",
    "#Correct Sentiment : 71\n",
    "Sentiment  precision: 0.0591\n",
    "Sentiment  recall: 0.3142\n",
    "Sentiment  F: 0.0995```\n",
    "\n",
    "#### FR\n",
    "\n",
    "```\n",
    "Pt2\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 1149\n",
    "\n",
    "#Correct Entity : 182\n",
    "Entity  precision: 0.1584\n",
    "Entity  recall: 0.8161\n",
    "Entity  F: 0.2653\n",
    "\n",
    "#Correct Sentiment : 68\n",
    "Sentiment  precision: 0.0592\n",
    "Sentiment  recall: 0.3049\n",
    "Sentiment  F: 0.0991 ```\n",
    "\n",
    "#### CN\n",
    "```\n",
    "#Entity in gold data: 362\n",
    "#Entity in prediction: 3318\n",
    "\n",
    "#Correct Entity : 183\n",
    "Entity  precision: 0.0552\n",
    "Entity  recall: 0.5055\n",
    "Entity  F: 0.0995\n",
    "\n",
    "#Correct Sentiment : 57\n",
    "Sentiment  precision: 0.0172\n",
    "Sentiment  recall: 0.1575\n",
    "Sentiment  F: 0.0310```\n",
    "\n",
    "#### SG\n",
    "```\n",
    "#Entity in gold data: 1382\n",
    "#Entity in prediction: 6599\n",
    "\n",
    "#Correct Entity : 794\n",
    "Entity  precision: 0.1203\n",
    "Entity  recall: 0.5745\n",
    "Entity  F: 0.1990\n",
    "\n",
    "#Correct Sentiment : 315\n",
    "Sentiment  precision: 0.0477\n",
    "Sentiment  recall: 0.2279\n",
    "Sentiment  F: 0.0789```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 3 first part \n",
    "'''\n",
    "first modify the train_data to get START and STOP in the data.\n",
    "train data = ['We A','are B','young C','Blank Blank','Hello D'] from read_file_train(fp+\"train\")\n",
    "test data = ['We','are','young','Blank','Hello'] from read_file_test(fp+\"dev.in\")\n",
    "Change to \n",
    "train data = ['START','We A','are B','young C','STOP','START','Hello D','STOP'] \n",
    "test data = ['START','We','are','young','STOP','START','Hello','STOP']\n",
    "'''\n",
    "def mod_data_for_transition(td,data_type):\n",
    "    print(\"Inserting START and STOP for \" + data_type +\"...\")\n",
    "    wtd = [] #words from train_data\n",
    "    if data_type == \"train\":\n",
    "        #extract the words first \n",
    "        for d in td:\n",
    "            w,t = d.rsplit(\" \",1) \n",
    "            wtd.append(t)\n",
    "    else:\n",
    "        wtd = td #don't need to extract \n",
    "    \n",
    "    #start replacing \n",
    "    for i in range(len(wtd)):\n",
    "        if wtd[i] == 'Blank':\n",
    "            index = i\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    wtd.insert(0,\"START\")\n",
    "    wtd[len(wtd)-1]=\"STOP\"\n",
    "    #TODO: am not sure why we need to do it twice. \n",
    "    for w in wtd:\n",
    "        if w == \"Blank\":\n",
    "            index = wtd.index(w)\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    return wtd \n",
    "    \n",
    "def build_dict_transition(td): \n",
    "    print(\"constructing dictionaries for transition...\")\n",
    "    '''\n",
    "    td=[\"START\",\"O\",\"O\",\"STOP\",\"START\",...]\n",
    "    1. build a tuple for (yi,yi-1)\n",
    "    2. put in dict \n",
    "    huge_count_yi_1_yi = {(yi,yi-1):1,...}\n",
    "    3. extract key[1] and put in huge_count_yi_1 \n",
    "    huge_count_yi_1 = {\"yi-1\":1,...}\n",
    "    '''\n",
    "    huge_count_yi_1_yi = {} #stores the no. of times of transition \n",
    "    huge_count_yi_1 = {} #stores no of times yi-1 occured \n",
    "#     print(len(td))\n",
    "    for i in range(1,len(td)):\n",
    "        # i = 0,1,2,...,n \n",
    "        temp = (td[i-1],td[i])\n",
    "        if temp not in huge_count_yi_1_yi:\n",
    "            huge_count_yi_1_yi[temp] = 1\n",
    "        else:\n",
    "            huge_count_yi_1_yi[temp] = huge_count_yi_1_yi[temp] + 1 \n",
    "#         print (temp)\n",
    "        if td[i-1] not in huge_count_yi_1:\n",
    "            huge_count_yi_1[td[i-1]] = 1\n",
    "        else:\n",
    "            huge_count_yi_1[td[i-1]] = huge_count_yi_1[td[i-1]] + 1\n",
    "            \n",
    "    return huge_count_yi_1_yi,huge_count_yi_1\n",
    "\n",
    "def calculate_transition(huge_count_yi_1_yi,huge_count_yi_1):\n",
    "    print(\"calculating transition probabilities...\")\n",
    "    '''\n",
    "    huge_transition = {(yi_1,yi):1,...}\n",
    "    '''\n",
    "    huge_transition = {}\n",
    "    for yi_1_yi,count_yi_1_yi in huge_count_yi_1_yi.items():\n",
    "        transition = 0.0 #ensure float\n",
    "        for yi_1,count_yi_1 in huge_count_yi_1.items(): \n",
    "            if yi_1 == yi_1_yi[0]:\n",
    "                transition = float(count_yi_1_yi / count_yi_1) #ensure float \n",
    "#                 transition = float(log(count_yi_1_yi) - log(count_yi_1))\n",
    "                huge_transition[yi_1_yi] = transition\n",
    "    return huge_transition\n",
    "\n",
    "#nightmare part 3 second part: viterbi algo.\n",
    "T = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral']\n",
    "def viterbi(emission,transition,td):  \n",
    "    print(\"Doing Viterbi V2...\")\n",
    "    massive_y_star = []  \n",
    "    #split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    seperated_sentence = split_sentence(td)\n",
    "    #START OF VITERBI#\n",
    "    #try one sentence:\n",
    "#     fs = seperated_sentence[1]\n",
    "\n",
    "    for fs in seperated_sentence:\n",
    "        all_pi = {}\n",
    "        y_star = []\n",
    "        for k in range(1,len(fs)-1): #ignore STOP; k = {1,...,n}\n",
    "            temp_all_tags_score = {}\n",
    "            for v in T:\n",
    "                max_pi_k_v = 0.0\n",
    "                temp_pi = {}\n",
    "                if k-1 == 0 :\n",
    "                    b = 0.0 \n",
    "                    a = 0.0 \n",
    "                    isBased = check_basecase(fs[k-1])\n",
    "                    if isBased:\n",
    "                        pi_prev = 1.0\n",
    "                    else:\n",
    "                        pi_prev = 0.0\n",
    "                    ekey = (fs[k],v)\n",
    "                    tkey = ('START',v)\n",
    "                    if ekey in emission:\n",
    "                        a = emission[ekey]\n",
    "                    if tkey in transition:\n",
    "                        b = transition[tkey]\n",
    "                    one_pi = pi_prev * b * a\n",
    "                    temp_pi[v] = one_pi\n",
    "                else:\n",
    "                    for u in T:\n",
    "                        b = 0.0 \n",
    "                        a = 0.0 \n",
    "                        ekey = (fs[k],v)\n",
    "                        tkey = (u,v)\n",
    "                        if ekey in emission:\n",
    "                            a = emission[ekey]\n",
    "                        if tkey in transition:\n",
    "                            b = transition[tkey]\n",
    "\n",
    "                        pi_prev = all_pi[k-1][u]\n",
    "                        one_pi = pi_prev * b * a\n",
    "                        temp_pi[u] = one_pi\n",
    "                temp_all_tags_score[v] =  max(temp_pi.values())\n",
    "            all_pi[k] = temp_all_tags_score\n",
    "\n",
    "        #for last case, yn -> yn+1 = STOP \n",
    "        n = len(fs) - 1 #this is n+1 \n",
    "        temp_pi_stop={}\n",
    "        temp_all_tags_score_stop = {}\n",
    "    #     print(\"Doing \",fs[k],\"->\",\"STOP\")\n",
    "        for v in T:\n",
    "            a = 0.0\n",
    "            tkey = (v,'STOP') #fs[k+1] = STOP\n",
    "            if tkey in transition:\n",
    "                a= transition[tkey]\n",
    "            pi_prev = all_pi[n-1][v]\n",
    "            one_pi = pi_prev * a\n",
    "            temp_pi_stop[v] = one_pi   \n",
    "        all_pi[n] = max(temp_pi_stop.values()) #add the value of STOP\n",
    "        \n",
    "        \n",
    "#         print(\"Doing backwards STOP\")\n",
    "        #backwards \n",
    "        y_star.append('STOP')\n",
    "        #finding tag for yn; yn -> stop\n",
    "        n = len(fs) - 2 # n in k = {1,...,n}\n",
    "        yn_star_values= {}\n",
    "        for v in T:\n",
    "            transition_value = 0.0\n",
    "            tkey = (v,'STOP') #fs[k+1] = STOP\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "            pi_current = all_pi[n][v] * transition_value\n",
    "            yn_star_values[v] = pi_current\n",
    "        yn_star = max(yn_star_values,key = yn_star_values.get)\n",
    "        y_star.insert(0,yn_star)\n",
    "\n",
    "#         print(\"Doing backwards\")\n",
    "        for n in range(len(fs)-3,0,-1): # 1 to n-1 in k={1,...,n}\n",
    "#             print(\"\\ndoing word: \",fs[n])\n",
    "            yn_1_star_values={}\n",
    "            for u in T:\n",
    "                transition_value = 0.0\n",
    "                tkey = (u,y_star[0]) \n",
    "\n",
    "                if tkey in transition:\n",
    "                    transition_value = transition[tkey]\n",
    "                pi_current = all_pi[n][u] * transition_value\n",
    "                yn_1_star_values[u] = pi_current\n",
    "            yn_1_star = max(yn_1_star_values, key = yn_1_star_values.get)\n",
    "            y_star.insert(0,yn_1_star)\n",
    "\n",
    "        y_star.insert(0,\"START\")\n",
    "#         print(y_star)\n",
    "        massive_y_star.append(y_star)\n",
    "    \n",
    "    massive_y_star = process_results(massive_y_star,seperated_sentence)\n",
    "    return massive_y_star\n",
    "\n",
    "def check_basecase(state):\n",
    "    if state=='START':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def mod_testdata(test_data,modified_train_data):\n",
    "    print(\"Replacing to #UNK# for test...\")\n",
    "    #extract the words \n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x,y = d.rsplit(\" \",1)\n",
    "        extracted_train_words.append(x)\n",
    "    #check whether the word exists in train. else replace with #UNK# \n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"START\" and w!= \"STOP\": #Avoid START and STOP.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    return test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 running...\n",
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing Viterbi V2...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n",
      "\n",
      "Doing FR/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing Viterbi V2...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n",
      "\n",
      "Doing CN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing Viterbi V2...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n",
      "\n",
      "Doing SG/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing Viterbi V2...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#part 3 \n",
    "print(\"Part 3 running...\")\n",
    "PART3_FN2 = \"dev.p33.out\"\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    # print(parsedtrainData)\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    stsp_train = (mod_data_for_transition(modifiedData,\"train\"))\n",
    "#     print(stsp_train)\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    \n",
    "    huge_count_yi_1_yi,huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "#     print(huge_count_yi_1)\n",
    "    huge_transition = calculate_transition(huge_count_yi_1_yi,huge_count_yi_1)\n",
    "#     print(huge_transition)\n",
    "    td = huge_transition\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    stsp_test = mod_data_for_transition(parsedtestData,\"test\")\n",
    "    modifiedTestdata = mod_testdata(stsp_test,modifiedData)\n",
    "#     print(modifiedTestdata)\n",
    "    massive_y_star =viterbi(huge_emission,huge_transition,modifiedTestdata)\n",
    "#     print(massive_y_star)\n",
    "    write_file(fp,PART3_FN,massive_y_star)\n",
    "    print(\"End...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TAGS = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral'] #ALL TAGS IN. \n",
    "\n",
    "#applicable to all cases \n",
    "def alpha_base(transition):\n",
    "#     print(\"Doing alpha base...\")\n",
    "    alpha_base = {}\n",
    "    for u in TAGS:\n",
    "        if ('START',u) in transition:\n",
    "            alpha_base[u] = transition[('START',u)]\n",
    "        else:\n",
    "            alpha_base[u] = 0.0\n",
    "    return alpha_base\n",
    "\n",
    "#tag u from Y node_no , where word is from Xn from the list of words \n",
    "def beta_base(transition,emission,sentence):\n",
    "#     print(\"Doing beta base...\")\n",
    "    X_n = sentence[len(sentence)-2] #last word \n",
    "    beta_base = {} \n",
    "    for u in TAGS:\n",
    "        transition_v = 0.0 \n",
    "        emission_v = 0.0 \n",
    "        if (u,'STOP') in transition:\n",
    "            transition_v = transition[(u,'STOP')]\n",
    "        if (X_n,u) in emission:\n",
    "            emission_v = emission[(X_n,u)]\n",
    "        beta_base[u] = float(transition_v * emission_v)\n",
    "    return beta_base\n",
    "\n",
    "def max_marginal(td,transition,emission,alpha_base):\n",
    "    print(\"Doing max marginal...\")\n",
    "    sentences = split_sentence(td)\n",
    "    MASSIVE_YSTAR=[]\n",
    "    for sentence in sentences:\n",
    "        betaBase = beta_base(transition,emission,sentence) #get the beta base \n",
    "        forward = {} \n",
    "        forward[1] = alpha_base #this should be constant\n",
    "        YSTAR = [] \n",
    "        #now do forward \n",
    "        for j in range(1,len(sentence)-1):\n",
    "            temp_alphas = {}\n",
    "            for u in TAGS:\n",
    "                alpha_j_1_v = 0.0 #value of alpha u (j+1) \n",
    "                for v in TAGS:\n",
    "                    a = 0.0 \n",
    "                    b = 0.0 \n",
    "                    if (v,u) in transition:\n",
    "                        a = transition[(v,u)]\n",
    "                    if (sentence[j],v) in emission:\n",
    "                        b = emission[(sentence[j],v)]\n",
    "                    alpha_j_1_v += float(forward[j][v] * a * b )\n",
    "                temp_alphas[u] = alpha_j_1_v\n",
    "            forward[j+1]= temp_alphas\n",
    "\n",
    "        #now do backward\n",
    "        backwards = {}\n",
    "        backwards[len(sentence)-1] = betaBase # inserting beta u n\n",
    "        for j in range(len(sentence)-2,0,-1): #j = n-1 ,...,1 \n",
    "            temp_beta={}\n",
    "            for u in TAGS: \n",
    "                beta_u_j = 0.0 \n",
    "                for v in TAGS:\n",
    "                    a = 0.0 \n",
    "                    b = 0.0 \n",
    "                    if (u,v) in transition:\n",
    "                        a = transition[(u,v)]\n",
    "                    if (sentence[j],u) in emission:\n",
    "                        b = emission[(sentence[j],u)]\n",
    "                    beta_u_j += float(backwards[j+1][v]*a*b)\n",
    "                temp_beta[u] = beta_u_j\n",
    "            backwards[j] = temp_beta\n",
    "\n",
    "        #find the tags\n",
    "        YSTAR.append(\"START\")\n",
    "        for j in range(1,len(sentence)-1):\n",
    "            temp = {}\n",
    "            for u in TAGS:\n",
    "                temp[u] = forward[j][u] * backwards[j][u]\n",
    "            YSTAR.append(max(temp,key=temp.get))\n",
    "        YSTAR.append(\"STOP\")\n",
    "        MASSIVE_YSTAR.append(YSTAR)\n",
    "    MASSIVE_YSTAR = process_results(MASSIVE_YSTAR,sentences)\n",
    "    return MASSIVE_YSTAR\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing max marginal...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n",
      "\n",
      "Doing FR/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing max marginal...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n",
      "\n",
      "Doing CN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing max marginal...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n",
      "\n",
      "Doing SG/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing max marginal...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#part 4\n",
    "print(\"Doing part 4...\")\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train(fp+\"train\")\n",
    "    modifiedData = modify_train_set(3,parsedtrainData)\n",
    "    stsp_train = (mod_data_for_transition(modifiedData,\"train\"))\n",
    "    huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "    huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "    huge_count_yi_1_yi,huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "    huge_transition = calculate_transition(huge_count_yi_1_yi,huge_count_yi_1)\n",
    "    parsedtestData = read_file_test(fp+\"dev.in\")\n",
    "    stsp_test = mod_data_for_transition(parsedtestData,\"test\")\n",
    "    modifiedTestdata = mod_testdata(stsp_test,modifiedData)\n",
    "    YSTAR = max_marginal(modifiedTestdata,huge_transition,huge_emission,alpha_base(huge_transition))\n",
    "    write_file(fp,PART4_FN,YSTAR)\n",
    "    print(\"End...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data file \n",
    "def read_file_train_p5(fp):\n",
    "    print(\"reading train file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "#             line = line.split(\" \") #lower case it \n",
    "#             line[0] = line[0].lower()\n",
    "#             line = line[0] + ' ' + line[1]\n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "#read the test data file \n",
    "def read_file_test_p5(fp):\n",
    "    print(\"reading tests file...\")\n",
    "    parsed = []\n",
    "    f = open(fp,'r',encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline() \n",
    "        #clean up the data read \n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n','Blank')) #Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "#             line = line.lower() #lower case \n",
    "            parsed.append(line.strip()) #remove \\n at the end \n",
    "        \n",
    "    return parsed\n",
    "\n",
    "#nightmare part 3 second part: viterbi algo.\n",
    "T = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral']\n",
    "def viterbi2(emission,transition,td):  \n",
    "    print(\"Doing Viterbi V2...\")\n",
    "    massive_y_star = []  \n",
    "    #split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    seperated_sentence = split_sentence(td)\n",
    "    #START OF VITERBI#\n",
    "    #try one sentence:\n",
    "    fs = seperated_sentence[1]\n",
    "\n",
    "#     for fs in seperated_sentence:\n",
    "    all_pi = {}\n",
    "    y_star = []\n",
    "    for k in range(1,len(fs)-1): #ignore STOP; k = {1,...,n}\n",
    "        temp_all_tags_score = {}\n",
    "        for v in T:\n",
    "            max_pi_k_v = 0.0\n",
    "            temp_pi = {}\n",
    "            if k-1 == 0 :\n",
    "                b = 0.0 \n",
    "                a = 0.0 \n",
    "                isBased = check_basecase(fs[k-1])\n",
    "                if isBased:\n",
    "                    pi_prev = 1.0\n",
    "                else:\n",
    "                    pi_prev = 0.0\n",
    "                ekey = (fs[k],v)\n",
    "                tkey = ('START',v)\n",
    "                if ekey in emission:\n",
    "                    a = emission[ekey]\n",
    "                if tkey in transition:\n",
    "                    b = transition[tkey]\n",
    "                one_pi = pi_prev * b * a\n",
    "                temp_pi[v] = one_pi\n",
    "            else:\n",
    "                for u in T:\n",
    "                    b = 0.0 \n",
    "                    a = 0.0 \n",
    "                    ekey = (fs[k],v)\n",
    "                    tkey = (u,v)\n",
    "                    if ekey in emission:\n",
    "                        a = emission[ekey]\n",
    "                    if tkey in transition:\n",
    "                        b = transition[tkey]\n",
    "\n",
    "                    pi_prev = all_pi[k-1][u]\n",
    "                    one_pi = pi_prev * b * a\n",
    "                    temp_pi[u] = one_pi\n",
    "            temp_all_tags_score[v] =  max(temp_pi.values())\n",
    "        all_pi[k] = temp_all_tags_score\n",
    "\n",
    "    #for last case, yn -> yn+1 = STOP \n",
    "    n = len(fs) - 1 #this is n+1 \n",
    "    temp_pi_stop={}\n",
    "    temp_all_tags_score_stop = {}\n",
    "#     print(\"Doing \",fs[k],\"->\",\"STOP\")\n",
    "    for v in T:\n",
    "        a = 0.0\n",
    "        tkey = (v,'STOP') #fs[k+1] = STOP\n",
    "        if tkey in transition:\n",
    "            a= transition[tkey]\n",
    "        pi_prev = all_pi[n-1][v]\n",
    "        one_pi = pi_prev * a\n",
    "        temp_pi_stop[v] = one_pi   \n",
    "    all_pi[n] = max(temp_pi_stop.values()) #add the value of STOP\n",
    "\n",
    "\n",
    "#         print(\"Doing backwards STOP\")\n",
    "    #backwards \n",
    "    y_star.append('STOP')\n",
    "    #finding tag for yn; yn -> stop\n",
    "    n = len(fs) - 2 # n in k = {1,...,n}\n",
    "    yn_star_values= {}\n",
    "    for v in T:\n",
    "        transition_value = 0.0\n",
    "        tkey = (v,'STOP') #fs[k+1] = STOP\n",
    "        if tkey in transition:\n",
    "            transition_value = transition[tkey]\n",
    "        pi_current = all_pi[n][v] * transition_value\n",
    "        yn_star_values[v] = pi_current\n",
    "    yn_star = max(yn_star_values,key = yn_star_values.get)\n",
    "    y_star.insert(0,yn_star)\n",
    "\n",
    "#         print(\"Doing backwards\")\n",
    "    for n in range(len(fs)-3,0,-1): # 1 to n-1 in k={1,...,n}\n",
    "#             print(\"\\ndoing word: \",fs[n])\n",
    "        yn_1_star_values={}\n",
    "        for u in T:\n",
    "            transition_value = 0.0\n",
    "            tkey = (u,y_star[0]) \n",
    "\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "            pi_current = all_pi[n][u] * transition_value\n",
    "            yn_1_star_values[u] = pi_current\n",
    "        yn_1_star = max(yn_1_star_values, key = yn_1_star_values.get)\n",
    "        y_star.insert(0,yn_1_star)\n",
    "\n",
    "    y_star.insert(0,\"START\")\n",
    "#         print(y_star)\n",
    "#         massive_y_star.append(y_star)\n",
    "    \n",
    "#     massive_y_star = process_results(massive_y_star,seperated_sentence)\n",
    "    return y_star\n",
    "\n",
    "def overall_p5():\n",
    "    '''\n",
    "    1. build the feature functions \n",
    "    {(we,O):0,(the, O):0}\n",
    "    f = [(we,O),(the,O),...]\n",
    "    init from test [we,the] for first round \n",
    "    viterbi produces f \n",
    "    w = [0] np.array(len(sentence))\n",
    "    feature function(x,y):\n",
    "    iterate through the function \n",
    "    finds the correct tuple in f \n",
    "    if is, take the index, then map it to w[index] * 1\n",
    "    else, w[index] * 0 \n",
    "    returns sum of log(w) + .... \n",
    "    2. finding the gold sentence \n",
    "    capture data from dev.out \n",
    "    gold data = [[sentences split into individual]]\n",
    "    sentences split into individual = [(we,O),...]\n",
    "    same weights \n",
    "    3. perceptron \n",
    "    boolean false \n",
    "    for each word, tag from viterbi:\n",
    "        if w[of that word and tag] <= 0 :\n",
    "            w = w + learning rate * (gold data sentence score - predicted sentence score )\n",
    "            put boolean = true\n",
    "    if true, run viterbi again \n",
    "    \n",
    "    4. viterbi (weight w,init f,transition)\n",
    "    start:\n",
    "    use transition like earlier on \n",
    "    log(start) + w1f1 \n",
    "    recursive case\n",
    "    for each tag v from u \n",
    "    find the score \n",
    "    score = feature function(word,u) + log(previous) \n",
    "    append \n",
    "    find max, the same \n",
    "    stop:\n",
    "    same concept with original via + log(transition) and selecting the maxmimum \n",
    "    \n",
    "    backwards:\n",
    "    yn -> yn+1 \n",
    "    -log(transition) \n",
    "    \n",
    "    '''\n",
    "\n",
    "def init_variables(gold,test,train): \n",
    "    print(\"Initialising variables\")\n",
    "    initF =[]\n",
    "    goldF =[]\n",
    "    trainF=[]\n",
    "    w = [0.0] * len(test)\n",
    "    for word in test:\n",
    "        initF.append(word)\n",
    "    for i in gold:\n",
    "        try:\n",
    "            word,tag = i.split(\" \")\n",
    "            goldF.append((word,tag))\n",
    "        except:\n",
    "            goldF.append(i)\n",
    "    for i in train:\n",
    "        try:\n",
    "            word,tag = i.split(\" \")\n",
    "            trainF.append((word,tag))\n",
    "        except:\n",
    "            trainF.append(i)\n",
    "    return initF,goldF,w,trainF\n",
    "\n",
    "def mod_data_init_p5(td,data_type):\n",
    "    print(\"Inserting START and STOP for \" + data_type +\"...\")\n",
    "    wtd = td\n",
    "    \n",
    "    #start replacing \n",
    "    for i in range(len(wtd)):\n",
    "        if wtd[i] == 'Blank':\n",
    "            index = i\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    wtd.insert(0,\"START\")\n",
    "    wtd[len(wtd)-1]=\"STOP\"\n",
    "    #TODO: am not sure why we need to do it twice. \n",
    "    for w in wtd:\n",
    "        if w == \"Blank\" or w.split(\" \")[0]==\"Blank\":\n",
    "            index = wtd.index(w)\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index+1,\"START\")\n",
    "    return wtd \n",
    "\n",
    "#w will be extracted from a vector of w, corresponding to the position \n",
    "#this will be done in viterbi \n",
    "def feature_function(word,v,trainF,w):\n",
    "    for i in trainF:\n",
    "        if word == i[0] and v = i[1]:\n",
    "            return w * 1.0\n",
    "    return 0.0\n",
    "\n",
    "#here, w will be the entire w vector \n",
    "def perceptron(predicted_ystar,gold_sentence,w):\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing part 5...\n",
      "\n",
      "Doing EN/...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "reading tests file...\n",
      "reading train file...\n",
      "Inserting START and STOP for train...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "Inserting START and STOP for GOLD...\n",
      "Inserting START and STOP for TEST...\n",
      "Replacing to #UNK# for test...\n",
      "Initialising variables\n",
      "End...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#part 5\n",
    "print(\"Doing part 5...\")\n",
    "PART5_FN = \"dev.p5.out\"\n",
    "for fp in FPList:\n",
    "    print(\"\\nDoing \" + fp + \"...\")\n",
    "    parsedtrainData = read_file_train_p5(fp+\"train\")\n",
    "    modifiedData = modify_train_set(0,parsedtrainData)\n",
    "    parsedtestData = read_file_test_p5(fp+\"dev.in\")\n",
    "    parsedgoldData =  read_file_train_p5(fp+\"dev.out\")\n",
    "    stsp_train = mod_data_for_transition(modifiedData,\"train\")\n",
    "    huge_count_yi_1_yi,huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "#     print(huge_count_yi_1)\n",
    "    huge_transition = calculate_transition(huge_count_yi_1_yi,huge_count_yi_1)\n",
    "    \n",
    "    stsp_gold = mod_data_init_p5(parsedgoldData,\"GOLD\")\n",
    "    stsp_test =  mod_data_init_p5(parsedtestData,\"TEST\")\n",
    "    modifiedTestdata = mod_testdata(stsp_test,modifiedData) #still replace the words we don't know with #UNK\n",
    "    initF,goldF,w,trainF = init_variables(stsp_gold,modifiedTestdata,stsp_train)\n",
    "#     print(w)\n",
    "#     write_file(fp,PART5_FN,YSTAR)\n",
    "    print(\"End...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log \n",
    "\n",
    "#try TF-IDF \n",
    "'''\n",
    "1 document = 1 sentence here \n",
    "'''\n",
    "\n",
    "def get_doc(data):\n",
    "    documents = [] \n",
    "    temp = []\n",
    "    for d in data:\n",
    "        if d.split(\" \")[0] == 'Blank':\n",
    "            documents.append(temp)\n",
    "            temp = [] \n",
    "        else:\n",
    "#             d = d.split(\" \")\n",
    "#             d[0] = d[0].lower()\n",
    "#             d = d[0] + \" \" + d[1]\n",
    "            temp.append(d)\n",
    "    totalDocument = len(documents) \n",
    "    return documents,totalDocument\n",
    "\n",
    "# def count_no_of_sentence(data,datatype):\n",
    "#     total = 0 \n",
    "#     if datatype == 'train':\n",
    "#         for d in data:\n",
    "#             if d.split(\" \")[0] == \"Blank\":\n",
    "#                 total += 1\n",
    "#     else:\n",
    "#         for w in data:\n",
    "#             if w == \"Blank\":\n",
    "#                 total += 1\n",
    "#     return total \n",
    "\n",
    "def tfidf(data,totalDocument,documents,wholeData):\n",
    "    #calculate df first \n",
    "    #init a dictionary of the word,tags =  0\n",
    "    \n",
    "    data_dict = {key: 0 for key in data} #df dictionary \n",
    "    #count the number of times the word,tag appears in one document \n",
    "    for document in documents:\n",
    "#         print(\"in doc\",document)\n",
    "        for k,v in data_dict.items():\n",
    "            if k in document:\n",
    "                data_dict[k] = v + 1 \n",
    "    \n",
    "    idf = {key: 0.0 for key in data}\n",
    "    for k,v in data_dict.items():\n",
    "        if v == 0 :\n",
    "            idf[k] = 0.0\n",
    "        else:\n",
    "            idf[k] = log(totalDocument/v,10)\n",
    "    tf = {key: 0.0 for key in data}   \n",
    "    for k in tf:\n",
    "        count = 0\n",
    "        for w in wholeData:\n",
    "            if w == k:\n",
    "                count += 1\n",
    "        tf[k] = 1+ log(count,10)\n",
    "    \n",
    "    tfidf_dict ={key: 0.0 for key in data}\n",
    "    for key in tfidf_dict:\n",
    "        tfidf_dict[key] = idf[key] * tf[key]\n",
    "        \n",
    "    return tfidf_dict\n",
    "\n",
    "def tagging(tfidf_dict,td):\n",
    "    tagged = [] \n",
    "#     print(tfidf)\n",
    "    for word in td:\n",
    "#         print(word)\n",
    "        temp = {}\n",
    "        for k in tfidf_dict:\n",
    "#             print(k.split(\" \")[0])\n",
    "            if k.split(' ')[0] == word:\n",
    "                temp[k] = tfidf_dict[k]\n",
    "        try :\n",
    "            tag = max(temp,key=temp.get).split(\" \")[1]\n",
    "#         print(tag)\n",
    "            tagged.append((word,tag))\n",
    "        except:\n",
    "            tagged.append((word,'O'))\n",
    "    return tagged\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
