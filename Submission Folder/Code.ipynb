{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Each cell corresponds to the respective part of the project. All helper functions needed are in their respective cells. So, to run the program for a particular part, change file paths as desired and just run the coresponding cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file...\n",
      "Reading input file...\n",
      "Modifying training data to #UNK#...\n",
      "Estimating emission probabilities...\n",
      "Cleaning up test file to replace unseen words with #UNK#...\n",
      "Writing output file...\n",
      "Output successful!\n"
     ]
    }
   ],
   "source": [
    "################# File Paths ####################\n",
    "output_path =   \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\\"\n",
    "output_file_name = \"dev.p3.out\"\n",
    "training_file = \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\train\"\n",
    "testing_file =  \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\\\Term 6\\Machine Learning\\\\Project\\\\EN\\\\dev.in\"\n",
    "\n",
    "################# Helper Functions ####################\n",
    "def read_file(file_path):\n",
    "    print(\"Reading input file...\")\n",
    "    file = open(file_path,'r',encoding=\"utf8\")\n",
    "    parsed_file = []\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if(line == ''): \n",
    "            break\n",
    "        elif(line == '\\n'):\n",
    "            parsed_file.append((None, None))\n",
    "            continue\n",
    "        else: \n",
    "            temp = line.replace('\\n','').split(' ')\n",
    "            parsed_file.append(tuple(temp)) #return a tuple of word and its tag\n",
    "      \n",
    "    return parsed_file\n",
    "\n",
    "def smartify(file_path):\n",
    "    training_data = read_file(file_path)\n",
    "    smart_data = {} #smart_data is basically training data but in a smart format \n",
    "                    #smart format is {tag:{word:frequency}}\n",
    "                    #example: {'O': {'We': 83, 'were': 125, 'then': 15, 'charged': 2, 'for': 279}\n",
    "    for word_tag_pair in training_data:\n",
    "        \n",
    "        if word_tag_pair==(None,None): #annoying case -.-\n",
    "            continue \n",
    "        \n",
    "        #if tag not in smart_data yet, add it \n",
    "        if not word_tag_pair[1] in smart_data.keys():\n",
    "            smart_data[word_tag_pair[1]]={}\n",
    "        \n",
    "        # if word not in smart_data[tag] yet, add it\n",
    "        if not word_tag_pair[0] in smart_data[word_tag_pair[1]].keys():\n",
    "            smart_data[word_tag_pair[1]][word_tag_pair[0]] = 1\n",
    "        \n",
    "        #increase frequency for each word-tag pair if seen\n",
    "        smart_data[word_tag_pair[1]][word_tag_pair[0]] = smart_data[word_tag_pair[1]][word_tag_pair[0]] +1\n",
    "    return smart_data\n",
    "\n",
    "def modify_training_set(training_data, k):\n",
    "    print(\"Modifying training data to #UNK#...\")\n",
    "    #input: smart_data\n",
    "    count = 0 #not sure if we will need number of words replaced by unk, but lets keep count anyway\n",
    "    \n",
    "    remove_words = []\n",
    "    training_data = training_data\n",
    "    for tag, word in training_data.items():\n",
    "        for specific_word, f in list(word.items()):\n",
    "            if (f<k):\n",
    "                count +=f\n",
    "#                 print(specific_word,f)\n",
    "                #replace with #unk#\n",
    "                word.pop(specific_word)\n",
    "                remove_words.append((tag,specific_word))\n",
    "        word[\"#UNK#\"] = count\n",
    "\n",
    "    return training_data\n",
    "\n",
    "import math\n",
    "def emission(file_path):\n",
    "    print(\"Reading input file...\")\n",
    "    \n",
    "    emissions = {}\n",
    "    training_data = smartify(file_path)\n",
    "    training_data = modify_training_set(training_data, k=3) #modify training data \n",
    "    \n",
    "    print(\"Estimating emission probabilities...\")\n",
    "    for tag, word in training_data.items():\n",
    "#         print(word.values(),\"\\n\")\n",
    "        count_y = sum(word.values())\n",
    "#         print (count_y)\n",
    "        for specific_word, f in word.items():\n",
    "#             print(specific_word,f)\n",
    "            temp_tup = (specific_word,tag)\n",
    "#             print(specific_word, f)\n",
    "            emissions[temp_tup] = float(f/(count_y))\n",
    "\n",
    "\n",
    "    return emissions\n",
    "\n",
    "def read_test_file(file_path):\n",
    "    print(\"Reading test file...\")\n",
    "    file = open(file_path,'r',encoding=\"utf8\")\n",
    "    parsed_file = []\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if(line == ''):\n",
    "            break\n",
    "        if(line == '\\n'):\n",
    "            parsed_file.append(None)\n",
    "            continue\n",
    "        parsed_file.append(line.replace('\\n',''))\n",
    "    return parsed_file\n",
    "\n",
    "def clean_up_test_data(test_data, emissions_dict):\n",
    "    \n",
    "    test_data = test_data\n",
    "#     print(\"Original test data:\",len(test_data))\n",
    "    \n",
    "    training_data = []\n",
    "    list_of_replaced_words = []\n",
    "    for tup in emissions_dict.keys():\n",
    "        training_data.append(tup[0])\n",
    "\n",
    "    #compare training and test data and replace non-common words with #UNK#\n",
    "    for word in test_data:\n",
    "        if word =='START' or word=='STOP':\n",
    "            continue\n",
    "        elif word not in training_data:\n",
    "            list_of_replaced_words.append(word) #we might need this for testing if the function is working\n",
    "            index = test_data.index(word)\n",
    "            test_data[index] = \"#UNK#\"\n",
    "\n",
    "\n",
    "    return test_data\n",
    "\n",
    "def construct_sentences(file_path):\n",
    "    file = open(file_path,'r',encoding = \"utf8\")\n",
    "    sentences = []\n",
    "    sentence = ['START']\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if(line == ''):\n",
    "            break\n",
    "        if(line == '\\n'):\n",
    "            sentence.append('STOP')\n",
    "            sentences.append(sentence)\n",
    "            sentence = ['START']\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(line.replace('\\n',''))\n",
    "            \n",
    "    return sentences \n",
    "\n",
    "################# Simple Sentiment Analysis ####################\n",
    "\n",
    "def sentiment_anal(train_file_path, test_file_path):\n",
    "#     test_data = read_test_file(test_file_path)\n",
    "    emissions = emission(train_file_path)\n",
    "\n",
    "    sentences = construct_sentences(test_file_path)\n",
    "    \n",
    "    print(\"Cleaning up test file to replace unseen words with #UNK#...\")\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = clean_up_test_data(sentences[i], emissions)\n",
    "\n",
    "    sentences_zipped = []\n",
    "    for sentence in sentences:\n",
    "        argmax = []\n",
    "        for word in sentence:\n",
    "            #         print(word)\n",
    "            if word == None:\n",
    "                argmax.append((None, None))\n",
    "                continue\n",
    "            temp = {}\n",
    "\n",
    "            # filter out common words into temp\n",
    "            for key in emissions.keys():\n",
    "\n",
    "                if (key[0] == word):\n",
    "                    #                 print(emissions[key])\n",
    "                    temp[key] = emissions[key]\n",
    "\n",
    "            if temp == {}:\n",
    "                continue\n",
    "            else:\n",
    "                result = max(temp, key=temp.get)\n",
    "            argmax.append(result)\n",
    "        sentences_zipped.append(argmax)\n",
    "    return sentences_zipped\n",
    "\n",
    "def write_dev_out(input_data, output_path):\n",
    "    print(\"Writing output file...\")\n",
    "    filename = output_path\n",
    "    with open(filename, 'w', encoding=\"utf8\") as f:\n",
    "        for i in range(len(input_data)):\n",
    "            if i != 0:\n",
    "                f.write(\"\\n\")\n",
    "            for tup in input_data[i]:\n",
    "                if (tup[0] == 'START' or tup[0] == 'STOP'):\n",
    "                    pass\n",
    "                else:\n",
    "                    f.write(tup[0] + ' ' + tup[1] + '\\n')\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "write_dev_out(sentiment_anal(training_file,testing_file),output_path+output_file_name)\n",
    "print(\"Output successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 running...\n",
      "reading train file...\n",
      "modifying to #UNK#...\n",
      "Inserting START and STOP for train...\n",
      "building count(y->x) & count(y)...\n",
      "calculating emission probability...\n",
      "constructing dictionaries for transition...\n",
      "calculating transition probabilities...\n",
      "reading tests file...\n",
      "Inserting START and STOP for test...\n",
      "Replacing to #UNK# for test...\n",
      "Doing Viterbi V2...\n",
      "Splitting sentence...\n",
      "writing file...\n",
      "End...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# File Paths ####################\n",
    "output_path =   \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\\"\n",
    "output_file_name = \"dev.p3.out\"\n",
    "training_file = \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\train\"\n",
    "testing_file =  \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\\\Term 6\\Machine Learning\\\\Project\\\\EN\\\\dev.in\"\n",
    "\n",
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "from math import log\n",
    "\n",
    "\n",
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    print(\"building count(y->x) & count(y)...\")\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    "\n",
    "    1. from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "\n",
    "    returns huge_count_y_x and huge_count_y \n",
    "    huge_count_y_x = {(\"We\",\"B-Positive\"):1}\n",
    "    huge_count_y = {\"B-Positive\":1}\n",
    "    '''\n",
    "\n",
    "    huge_count_y_x = {}  # dict to contain all count(y->x)\n",
    "    huge_count_y = {}  # dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x, y = d.split(\" \")  # split by spaces \n",
    "        temp_tuple = (x, y)\n",
    "\n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1\n",
    "\n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] = 1\n",
    "        else:\n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x, huge_count_y\n",
    "\n",
    "\n",
    "# part 2 first part \n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    print(\"calculating emission probability...\")\n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission = {}  # dict to contain all the emission probabilities \n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        emission = 0.0  # if none found, then return as 0 \n",
    "        tag = seq[1]\n",
    "        #         print(k,v)\n",
    "        count_y = huge_count_y[tag]\n",
    "        emission = float(count_y_x / count_y)\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "\n",
    "\n",
    "def modify_train_set(k, train_data):\n",
    "    print(\"modifying to #UNK#...\")\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "        #         print(td)\n",
    "        word, tag = td.rsplit(\" \", 1)\n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    # identify which words appeared less than k times \n",
    "    list_of_works_less_than_k = []\n",
    "    for w, count in count_dict.items():\n",
    "        if count < k and w != \"Blank\":  # Skip blanks. \n",
    "            list_of_works_less_than_k.append(w)\n",
    "    # now replace the entries in the training set  \n",
    "    for td in train_data:\n",
    "        word, tag = td.rsplit(\" \", 1)\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "# read the training data file\n",
    "def read_file_train(fp):\n",
    "    print(\"reading train file...\")\n",
    "    parsed = []\n",
    "    f = open(fp, 'r', encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        # clean up the data read\n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n', 'Blank Blank'))  # Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip())  # remove \\n at the end\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "# read the test data file\n",
    "def read_file_test(fp):\n",
    "    print(\"reading tests file...\")\n",
    "    parsed = []\n",
    "    f = open(fp, 'r', encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        # clean up the data read\n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n', 'Blank'))  # Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip())  # remove \\n at the end\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "\n",
    "\n",
    "def write_file(fp, fn, data):\n",
    "    print(\"writing file...\")\n",
    "    with open(fp + fn, 'w', encoding='utf8') as f:\n",
    "        for k in data:\n",
    "            if k[0] == \"Blank\":  # replace the blanks\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(k[0] + ' ' + k[1] + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# returns in (word,tag), with START STOP replaced, Blank returns.\n",
    "def process_results(massive_y_star, seperated_sentence):\n",
    "    tagged_words = []  # return this\n",
    "    for i in range(len(massive_y_star)):\n",
    "        for j in range(len(massive_y_star[i])):\n",
    "            if massive_y_star[i][j] == \"START\":\n",
    "                continue  # don't do anything\n",
    "            elif massive_y_star[i][j] == \"STOP\":\n",
    "                # replace to blank blank\n",
    "                temp = ('Blank', 'Blank')\n",
    "                tagged_words.append(temp)\n",
    "            else:  # normal words\n",
    "                temp = (seperated_sentence[i][j], massive_y_star[i][j])\n",
    "                tagged_words.append(temp)\n",
    "    return tagged_words\n",
    "\n",
    "\n",
    "def split_sentence(td):\n",
    "    print(\"Splitting sentence...\")\n",
    "    # split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    temp = []\n",
    "    for word in td:\n",
    "        if word != \"STOP\":\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(\"STOP\")\n",
    "            seperated_sentence.append(temp)\n",
    "            temp = []\n",
    "    return seperated_sentence\n",
    "\n",
    "\n",
    "'''\n",
    "first modify the train_data to get START and STOP in the data.\n",
    "train data = ['We A','are B','young C','Blank Blank','Hello D'] from read_file_train(fp+\"train\")\n",
    "test data = ['We','are','young','Blank','Hello'] from read_file_test(fp+\"dev.in\")\n",
    "Change to \n",
    "train data = ['START','We A','are B','young C','STOP','START','Hello D','STOP'] \n",
    "test data = ['START','We','are','young','STOP','START','Hello','STOP']\n",
    "'''\n",
    "\n",
    "\n",
    "def mod_data_for_transition(td, data_type):\n",
    "    print(\"Inserting START and STOP for \" + data_type + \"...\")\n",
    "    wtd = []  # words from train_data\n",
    "    if data_type == \"train\":\n",
    "        # extract the words first\n",
    "        for d in td:\n",
    "            w, t = d.rsplit(\" \", 1)\n",
    "            wtd.append(t)\n",
    "    else:\n",
    "        wtd = td  # don't need to extract\n",
    "\n",
    "    # start replacing\n",
    "    for i in range(len(wtd)):\n",
    "        if wtd[i] == 'Blank':\n",
    "            index = i\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index + 1, \"START\")\n",
    "    wtd.insert(0, \"START\")\n",
    "    wtd[len(wtd) - 1] = \"STOP\"\n",
    "    # TODO: am not sure why we need to do it twice.\n",
    "    for w in wtd:\n",
    "        if w == \"Blank\":\n",
    "            index = wtd.index(w)\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index + 1, \"START\")\n",
    "    return wtd\n",
    "\n",
    "\n",
    "def build_dict_transition(td):\n",
    "    print(\"constructing dictionaries for transition...\")\n",
    "    '''\n",
    "    td=[\"START\",\"O\",\"O\",\"STOP\",\"START\",...]\n",
    "    1. build a tuple for (yi,yi-1)\n",
    "    2. put in dict \n",
    "    huge_count_yi_1_yi = {(yi,yi-1):1,...}\n",
    "    3. extract key[1] and put in huge_count_yi_1 \n",
    "    huge_count_yi_1 = {\"yi-1\":1,...}\n",
    "    '''\n",
    "    huge_count_yi_1_yi = {}  # stores the no. of times of transition\n",
    "    huge_count_yi_1 = {}  # stores no of times yi-1 occured\n",
    "    #     print(len(td))\n",
    "    for i in range(1, len(td)):\n",
    "        # i = 0,1,2,...,n\n",
    "        temp = (td[i - 1], td[i])\n",
    "        if temp not in huge_count_yi_1_yi:\n",
    "            huge_count_yi_1_yi[temp] = 1\n",
    "        else:\n",
    "            huge_count_yi_1_yi[temp] = huge_count_yi_1_yi[temp] + 1\n",
    "        # print (temp)\n",
    "        if td[i - 1] not in huge_count_yi_1:\n",
    "            huge_count_yi_1[td[i - 1]] = 1\n",
    "        else:\n",
    "            huge_count_yi_1[td[i - 1]] = huge_count_yi_1[td[i - 1]] + 1\n",
    "\n",
    "    return huge_count_yi_1_yi, huge_count_yi_1\n",
    "\n",
    "\n",
    "def calculate_transition(huge_count_yi_1_yi, huge_count_yi_1):\n",
    "    print(\"calculating transition probabilities...\")\n",
    "    '''\n",
    "    huge_transition = {(yi_1,yi):1,...}\n",
    "    '''\n",
    "    huge_transition = {}\n",
    "    for yi_1_yi, count_yi_1_yi in huge_count_yi_1_yi.items():\n",
    "        transition = 0.0  # ensure float\n",
    "        for yi_1, count_yi_1 in huge_count_yi_1.items():\n",
    "            if yi_1 == yi_1_yi[0]:\n",
    "                transition = float(count_yi_1_yi / count_yi_1)  # ensure float\n",
    "                #                 transition = float(log(count_yi_1_yi) - log(count_yi_1))\n",
    "                huge_transition[yi_1_yi] = transition\n",
    "    return huge_transition\n",
    "\n",
    "\n",
    "# nightmare part 3 second part: viterbi algo.\n",
    "T = ['O', 'B-positive', 'B-neutral', 'B-negative', 'I-positive', 'I-negative', 'I-neutral']\n",
    "\n",
    "\n",
    "def viterbi(emission, transition, td):\n",
    "    print(\"Doing Viterbi V2...\")\n",
    "    massive_y_star = []\n",
    "    # split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    seperated_sentence = split_sentence(td)\n",
    "    # START OF VITERBI#\n",
    "    # try one sentence:\n",
    "    #     fs = seperated_sentence[1]\n",
    "\n",
    "    for fs in seperated_sentence:\n",
    "        all_pi = {}\n",
    "        y_star = []\n",
    "        for k in range(1, len(fs) - 1):  # ignore STOP; k = {1,...,n}\n",
    "            temp_all_tags_score = {}\n",
    "            for v in T:\n",
    "                max_pi_k_v = 0.0\n",
    "                temp_pi = {}\n",
    "                if k - 1 == 0:\n",
    "                    b = 0.0\n",
    "                    a = 0.0\n",
    "                    isBased = check_basecase(fs[k - 1])\n",
    "                    if isBased:\n",
    "                        pi_prev = 1.0\n",
    "                    else:\n",
    "                        pi_prev = 0.0\n",
    "                    ekey = (fs[k], v)\n",
    "                    tkey = ('START', v)\n",
    "                    if ekey in emission:\n",
    "                        a = emission[ekey]\n",
    "                    if tkey in transition:\n",
    "                        b = transition[tkey]\n",
    "                    one_pi = pi_prev * b * a\n",
    "                    temp_pi[v] = one_pi\n",
    "                else:\n",
    "                    for u in T:\n",
    "                        b = 0.0\n",
    "                        a = 0.0\n",
    "                        ekey = (fs[k], v)\n",
    "                        tkey = (u, v)\n",
    "                        if ekey in emission:\n",
    "                            a = emission[ekey]\n",
    "                        if tkey in transition:\n",
    "                            b = transition[tkey]\n",
    "\n",
    "                        pi_prev = all_pi[k - 1][u]\n",
    "                        one_pi = pi_prev * b * a\n",
    "                        temp_pi[u] = one_pi\n",
    "                temp_all_tags_score[v] = max(temp_pi.values())\n",
    "            all_pi[k] = temp_all_tags_score\n",
    "\n",
    "        # for last case, yn -> yn+1 = STOP\n",
    "        n = len(fs) - 1  # this is n+1\n",
    "        temp_pi_stop = {}\n",
    "        temp_all_tags_score_stop = {}\n",
    "        #     print(\"Doing \",fs[k],\"->\",\"STOP\")\n",
    "        for v in T:\n",
    "            a = 0.0\n",
    "            tkey = (v, 'STOP')  # fs[k+1] = STOP\n",
    "            if tkey in transition:\n",
    "                a = transition[tkey]\n",
    "            pi_prev = all_pi[n - 1][v]\n",
    "            one_pi = pi_prev * a\n",
    "            temp_pi_stop[v] = one_pi\n",
    "        all_pi[n] = max(temp_pi_stop.values())  # add the value of STOP\n",
    "\n",
    "        #         print(\"Doing backwards STOP\")\n",
    "        # backwards\n",
    "        y_star.append('STOP')\n",
    "        # finding tag for yn; yn -> stop\n",
    "        n = len(fs) - 2  # n in k = {1,...,n}\n",
    "        yn_star_values = {}\n",
    "        for v in T:\n",
    "            transition_value = 0.0\n",
    "            tkey = (v, 'STOP')  # fs[k+1] = STOP\n",
    "            if tkey in transition:\n",
    "                transition_value = transition[tkey]\n",
    "            pi_current = all_pi[n][v] * transition_value\n",
    "            yn_star_values[v] = pi_current\n",
    "        yn_star = max(yn_star_values, key=yn_star_values.get)\n",
    "        y_star.insert(0, yn_star)\n",
    "\n",
    "        #         print(\"Doing backwards\")\n",
    "        for n in range(len(fs) - 3, 0, -1):  # 1 to n-1 in k={1,...,n}\n",
    "            #             print(\"\\ndoing word: \",fs[n])\n",
    "            yn_1_star_values = {}\n",
    "            for u in T:\n",
    "                transition_value = 0.0\n",
    "                tkey = (u, y_star[0])\n",
    "\n",
    "                if tkey in transition:\n",
    "                    transition_value = transition[tkey]\n",
    "                pi_current = all_pi[n][u] * transition_value\n",
    "                yn_1_star_values[u] = pi_current\n",
    "            yn_1_star = max(yn_1_star_values, key=yn_1_star_values.get)\n",
    "            y_star.insert(0, yn_1_star)\n",
    "\n",
    "        y_star.insert(0, \"START\")\n",
    "        #         print(y_star)\n",
    "        massive_y_star.append(y_star)\n",
    "\n",
    "    massive_y_star = process_results(massive_y_star, seperated_sentence)\n",
    "    return massive_y_star\n",
    "\n",
    "\n",
    "def check_basecase(state):\n",
    "    if state == 'START':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def mod_testdata(test_data, modified_train_data):\n",
    "    print(\"Replacing to #UNK# for test...\")\n",
    "    # extract the words\n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x, y = d.rsplit(\" \", 1)\n",
    "        extracted_train_words.append(x)\n",
    "    # check whether the word exists in train. else replace with #UNK#\n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"START\" and w != \"STOP\":  # Avoid START and STOP.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    return test_data\n",
    "\n",
    "\n",
    "# part 3\n",
    "print(\"Part 3 running...\")\n",
    "\n",
    "# print(\"\\nDoing \" + fp + \"...\")\n",
    "parsedtrainData = read_file_train(training_file)\n",
    "modifiedData = modify_train_set(3, parsedtrainData)\n",
    "stsp_train = (mod_data_for_transition(modifiedData, \"train\"))\n",
    "huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "huge_count_yi_1_yi, huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "huge_transition = calculate_transition(huge_count_yi_1_yi, huge_count_yi_1)#     print(huge_transition)\n",
    "td = huge_transition\n",
    "parsedtestData = read_file_test(testing_file)\n",
    "stsp_test = mod_data_for_transition(parsedtestData, \"test\")\n",
    "modifiedTestdata = mod_testdata(stsp_test, modifiedData)\n",
    "massive_y_star = viterbi(huge_emission, huge_transition, modifiedTestdata)\n",
    "write_file(output_path, output_file_name, massive_y_star)\n",
    "print(\"End...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# File Paths ####################\n",
    "output_path =   \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\\"\n",
    "output_file_name = \"dev.p4.out\"\n",
    "training_file = \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\train\"\n",
    "testing_file =  \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\\\Term 6\\Machine Learning\\\\Project\\\\EN\\\\dev.in\"\n",
    "\n",
    "'''\n",
    "General functions\n",
    "read & write requires encoding='utf8' due to different languages in the files.\n",
    "'''\n",
    "from math import log\n",
    "\n",
    "\n",
    "# build dictionaries of count(y->x) and count(y)\n",
    "def built_dict(train_data):\n",
    "    print(\"building count(y->x) & count(y)...\")\n",
    "    '''\n",
    "    data will be a array of the read train data \n",
    "    1. create tuples:count out of the y and x and put them in global_y_x\n",
    "    2. if the tuples were repeated, don't add a new entry into the dictionary but increase the count \n",
    "\n",
    "    1. from global_y_x, we have gotten count(y-> x) . \n",
    "    2. need get count(y)\n",
    "\n",
    "    returns huge_count_y_x and huge_count_y \n",
    "    huge_count_y_x = {(\"We\",\"B-Positive\"):1}\n",
    "    huge_count_y = {\"B-Positive\":1}\n",
    "    '''\n",
    "\n",
    "    huge_count_y_x = {}  # dict to contain all count(y->x)\n",
    "    huge_count_y = {}  # dict to contain all count(y)\n",
    "    for d in train_data:\n",
    "        x, y = d.split(\" \")  # split by spaces\n",
    "        temp_tuple = (x, y)\n",
    "\n",
    "        if temp_tuple not in huge_count_y_x:\n",
    "            huge_count_y_x[temp_tuple] = 1\n",
    "        else:\n",
    "            huge_count_y_x[temp_tuple] = huge_count_y_x[temp_tuple] + 1\n",
    "\n",
    "        if y not in huge_count_y:\n",
    "            huge_count_y[y] = 1\n",
    "        else:\n",
    "            huge_count_y[y] = huge_count_y[y] + 1\n",
    "    return huge_count_y_x, huge_count_y\n",
    "\n",
    "\n",
    "# part 2 first part\n",
    "\n",
    "def calculate_emmision(huge_count_y_x, huge_count_y):\n",
    "    print(\"calculating emission probability...\")\n",
    "    '''\n",
    "    Calculating the emission \n",
    "    Use the tuple (x,y) to represent the occurance of e(x|y)  \n",
    "    '''\n",
    "    huge_emission = {}  # dict to contain all the emission probabilities\n",
    "    for seq, count_y_x in huge_count_y_x.items():\n",
    "        emission = 0.0  # if none found, then return as 0\n",
    "        tag = seq[1]\n",
    "        #         print(k,v)\n",
    "        count_y = huge_count_y[tag]\n",
    "        emission = float(count_y_x / count_y)\n",
    "        huge_emission[seq] = emission\n",
    "\n",
    "    return huge_emission\n",
    "\n",
    "'''\n",
    "    replace words that appear < k times with #UNK# \n",
    "    modify_train_set accepts k, training data\n",
    "    training data is a list returned from read_file_train\n",
    "'''\n",
    "\n",
    "\n",
    "def modify_train_set(k, train_data):\n",
    "    print(\"modifying to #UNK#...\")\n",
    "    modified_training_data = []\n",
    "    count_dict = {}\n",
    "    for td in train_data:\n",
    "        #         print(td)\n",
    "        word, tag = td.rsplit(\" \", 1)\n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "    # identify which words appeared less than k times\n",
    "    list_of_works_less_than_k = []\n",
    "    for w, count in count_dict.items():\n",
    "        if count < k and w != \"Blank\":  # Skip blanks.\n",
    "            list_of_works_less_than_k.append(w)\n",
    "    # now replace the entries in the training set\n",
    "    for td in train_data:\n",
    "        word, tag = td.rsplit(\" \", 1)\n",
    "        for words_to_be_removed in list_of_works_less_than_k:\n",
    "            if word == words_to_be_removed:\n",
    "                index = train_data.index(td)\n",
    "                train_data[index] = '#UNK# ' + tag\n",
    "    return train_data\n",
    "# read the training data file\n",
    "def read_file_train(fp):\n",
    "    print(\"reading train file...\")\n",
    "    parsed = []\n",
    "    f = open(fp, 'r', encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        # clean up the data read\n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n', 'Blank Blank'))  # Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip())  # remove \\n at the end\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "# read the test data file\n",
    "def read_file_test(fp):\n",
    "    print(\"reading tests file...\")\n",
    "    parsed = []\n",
    "    f = open(fp, 'r', encoding='utf8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        # clean up the data read\n",
    "        if line == '\\n':\n",
    "            parsed.append(line.replace('\\n', 'Blank'))  # Replace spaces with Blanks.\n",
    "        elif line == \"\":\n",
    "            break\n",
    "        else:\n",
    "            parsed.append(line.strip())  # remove \\n at the end\n",
    "\n",
    "    return parsed\n",
    "\n",
    "def mod_testdata(test_data, modified_train_data):\n",
    "    print(\"Replacing to #UNK# for test...\")\n",
    "    # extract the words\n",
    "    extracted_train_words = []\n",
    "    for d in modified_train_data:\n",
    "        x, y = d.rsplit(\" \", 1)\n",
    "        extracted_train_words.append(x)\n",
    "    # check whether the word exists in train. else replace with #UNK#\n",
    "    for w in test_data:\n",
    "        if w not in extracted_train_words and w != \"START\" and w != \"STOP\":  # Avoid START and STOP.\n",
    "            index = test_data.index(w)\n",
    "            test_data[index] = '#UNK#'\n",
    "    return test_data\n",
    "\n",
    "'''\n",
    "data received shall be in dictionary term of \n",
    "data = ((x1 ,y),(x2,y),...)\n",
    "fn = filename \n",
    "fp = filepath\n",
    "'''\n",
    "\n",
    "\n",
    "def write_file(fp, fn, data):\n",
    "    print(\"writing file...\")\n",
    "    with open(fp + fn, 'w', encoding='utf8') as f:\n",
    "        for k in data:\n",
    "            if k[0] == \"Blank\":  # replace the blanks\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(k[0] + ' ' + k[1] + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# returns in (word,tag), with START STOP replaced, Blank returns.\n",
    "def process_results(massive_y_star, seperated_sentence):\n",
    "    tagged_words = []  # return this\n",
    "    for i in range(len(massive_y_star)):\n",
    "        for j in range(len(massive_y_star[i])):\n",
    "            if massive_y_star[i][j] == \"START\":\n",
    "                continue  # don't do anything\n",
    "            elif massive_y_star[i][j] == \"STOP\":\n",
    "                # replace to blank blank\n",
    "                temp = ('Blank', 'Blank')\n",
    "                tagged_words.append(temp)\n",
    "            else:  # normal words\n",
    "                temp = (seperated_sentence[i][j], massive_y_star[i][j])\n",
    "                tagged_words.append(temp)\n",
    "    return tagged_words\n",
    "\n",
    "\n",
    "def split_sentence(td):\n",
    "    print(\"Splitting sentence...\")\n",
    "    # split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    temp = []\n",
    "    for word in td:\n",
    "        if word != \"STOP\":\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(\"STOP\")\n",
    "            seperated_sentence.append(temp)\n",
    "            temp = []\n",
    "    return seperated_sentence\n",
    "\n",
    "\n",
    "'''\n",
    "first modify the train_data to get START and STOP in the data.\n",
    "train data = ['We A','are B','young C','Blank Blank','Hello D'] from read_file_train(fp+\"train\")\n",
    "test data = ['We','are','young','Blank','Hello'] from read_file_test(fp+\"dev.in\")\n",
    "Change to \n",
    "train data = ['START','We A','are B','young C','STOP','START','Hello D','STOP'] \n",
    "test data = ['START','We','are','young','STOP','START','Hello','STOP']\n",
    "'''\n",
    "\n",
    "\n",
    "def mod_data_for_transition(td, data_type):\n",
    "    print(\"Inserting START and STOP for \" + data_type + \"...\")\n",
    "    wtd = []  # words from train_data\n",
    "    if data_type == \"train\":\n",
    "        # extract the words first\n",
    "        for d in td:\n",
    "            w, t = d.rsplit(\" \", 1)\n",
    "            wtd.append(t)\n",
    "    else:\n",
    "        wtd = td  # don't need to extract\n",
    "\n",
    "    # start replacing\n",
    "    for i in range(len(wtd)):\n",
    "        if wtd[i] == 'Blank':\n",
    "            index = i\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index + 1, \"START\")\n",
    "    wtd.insert(0, \"START\")\n",
    "    wtd[len(wtd) - 1] = \"STOP\"\n",
    "    # TODO: am not sure why we need to do it twice.\n",
    "    for w in wtd:\n",
    "        if w == \"Blank\":\n",
    "            index = wtd.index(w)\n",
    "            wtd[index] = \"STOP\"\n",
    "            wtd.insert(index + 1, \"START\")\n",
    "    return wtd\n",
    "\n",
    "\n",
    "def build_dict_transition(td):\n",
    "    print(\"constructing dictionaries for transition...\")\n",
    "    '''\n",
    "    td=[\"START\",\"O\",\"O\",\"STOP\",\"START\",...]\n",
    "    1. build a tuple for (yi,yi-1)\n",
    "    2. put in dict \n",
    "    huge_count_yi_1_yi = {(yi,yi-1):1,...}\n",
    "    3. extract key[1] and put in huge_count_yi_1 \n",
    "    huge_count_yi_1 = {\"yi-1\":1,...}\n",
    "    '''\n",
    "    huge_count_yi_1_yi = {}  # stores the no. of times of transition\n",
    "    huge_count_yi_1 = {}  # stores no of times yi-1 occured\n",
    "    #     print(len(td))\n",
    "    for i in range(1, len(td)):\n",
    "        # i = 0,1,2,...,n\n",
    "        temp = (td[i - 1], td[i])\n",
    "        if temp not in huge_count_yi_1_yi:\n",
    "            huge_count_yi_1_yi[temp] = 1\n",
    "        else:\n",
    "            huge_count_yi_1_yi[temp] = huge_count_yi_1_yi[temp] + 1\n",
    "        # print (temp)\n",
    "        if td[i - 1] not in huge_count_yi_1:\n",
    "            huge_count_yi_1[td[i - 1]] = 1\n",
    "        else:\n",
    "            huge_count_yi_1[td[i - 1]] = huge_count_yi_1[td[i - 1]] + 1\n",
    "\n",
    "    return huge_count_yi_1_yi, huge_count_yi_1\n",
    "\n",
    "\n",
    "def calculate_transition(huge_count_yi_1_yi, huge_count_yi_1):\n",
    "    print(\"calculating transition probabilities...\")\n",
    "    '''\n",
    "    huge_transition = {(yi_1,yi):1,...}\n",
    "    '''\n",
    "    huge_transition = {}\n",
    "    for yi_1_yi, count_yi_1_yi in huge_count_yi_1_yi.items():\n",
    "        transition = 0.0  # ensure float\n",
    "        for yi_1, count_yi_1 in huge_count_yi_1.items():\n",
    "            if yi_1 == yi_1_yi[0]:\n",
    "                transition = float(count_yi_1_yi / count_yi_1)  # ensure float\n",
    "                #                 transition = float(log(count_yi_1_yi) - log(count_yi_1))\n",
    "                huge_transition[yi_1_yi] = transition\n",
    "    return huge_transition\n",
    "\n",
    "\n",
    "TAGS = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral'] #ALL TAGS IN.\n",
    "#returns in (word,tag), with START STOP replaced, Blank returns.\n",
    "def process_results(massive_y_star,seperated_sentence):\n",
    "    tagged_words = [] #return this\n",
    "    for i in range(len(massive_y_star)):\n",
    "        for j in range(len(massive_y_star[i])):\n",
    "            if massive_y_star[i][j] == \"START\":\n",
    "                continue #don't do anything\n",
    "            elif massive_y_star[i][j] == \"STOP\":\n",
    "                #replace to blank blank\n",
    "                temp = ('Blank','Blank')\n",
    "                tagged_words.append(temp)\n",
    "            else: #normal words\n",
    "                temp = (seperated_sentence[i][j],massive_y_star[i][j])\n",
    "                tagged_words.append(temp)\n",
    "    return tagged_words\n",
    "\n",
    "def split_sentence(td):\n",
    "    print(\"Splitting sentence...\")\n",
    "    #split into individual sentences\n",
    "    seperated_sentence = []\n",
    "    temp = []\n",
    "    for word in td:\n",
    "        if word != \"STOP\":\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(\"STOP\")\n",
    "            seperated_sentence.append(temp)\n",
    "            temp =[]\n",
    "    return seperated_sentence\n",
    "#applicable to all cases \n",
    "def alpha_base(transition):\n",
    "#     print(\"Doing alpha base...\")\n",
    "    alpha_base = {}\n",
    "    for u in TAGS:\n",
    "        if ('START',u) in transition:\n",
    "            alpha_base[u] = transition[('START',u)]\n",
    "        else:\n",
    "            alpha_base[u] = 0.0\n",
    "    return alpha_base\n",
    "\n",
    "#tag u from Y node_no , where word is from Xn from the list of words \n",
    "def beta_base(transition,emission,sentence):\n",
    "#     print(\"Doing beta base...\")\n",
    "    X_n = sentence[len(sentence)-2] #last word \n",
    "    beta_base = {} \n",
    "    for u in TAGS:\n",
    "        transition_v = 0.0 \n",
    "        emission_v = 0.0 \n",
    "        if (u,'STOP') in transition:\n",
    "            transition_v = transition[(u,'STOP')]\n",
    "        if (X_n,u) in emission:\n",
    "            emission_v = emission[(X_n,u)]\n",
    "        beta_base[u] = float(transition_v * emission_v)\n",
    "    return beta_base\n",
    "\n",
    "def max_marginal(td,transition,emission,alpha_base):\n",
    "    print(\"Doing max marginal...\")\n",
    "    sentences = split_sentence(td)\n",
    "    MASSIVE_YSTAR=[]\n",
    "    for sentence in sentences:\n",
    "        betaBase = beta_base(transition,emission,sentence) #get the beta base \n",
    "        forward = {} \n",
    "        forward[1] = alpha_base #this should be constant\n",
    "        YSTAR = [] \n",
    "        #now do forward \n",
    "        for j in range(1,len(sentence)-1):\n",
    "            temp_alphas = {}\n",
    "            for u in TAGS:\n",
    "                alpha_j_1_v = 0.0 #value of alpha u (j+1) \n",
    "                for v in TAGS:\n",
    "                    a = 0.0 \n",
    "                    b = 0.0 \n",
    "                    if (v,u) in transition:\n",
    "                        a = transition[(v,u)]\n",
    "                    if (sentence[j],v) in emission:\n",
    "                        b = emission[(sentence[j],v)]\n",
    "                    alpha_j_1_v += float(forward[j][v] * a * b )\n",
    "                temp_alphas[u] = alpha_j_1_v\n",
    "            forward[j+1]= temp_alphas\n",
    "\n",
    "        #now do backward\n",
    "        backwards = {}\n",
    "        backwards[len(sentence)-1] = betaBase # inserting beta u n\n",
    "        for j in range(len(sentence)-2,0,-1): #j = n-1 ,...,1 \n",
    "            temp_beta={}\n",
    "            for u in TAGS: \n",
    "                beta_u_j = 0.0 \n",
    "                for v in TAGS:\n",
    "                    a = 0.0 \n",
    "                    b = 0.0 \n",
    "                    if (u,v) in transition:\n",
    "                        a = transition[(u,v)]\n",
    "                    if (sentence[j],u) in emission:\n",
    "                        b = emission[(sentence[j],u)]\n",
    "                    beta_u_j += float(backwards[j+1][v]*a*b)\n",
    "                temp_beta[u] = beta_u_j\n",
    "            backwards[j] = temp_beta\n",
    "\n",
    "        #find the tags\n",
    "        YSTAR.append(\"START\")\n",
    "        for j in range(1,len(sentence)-1):\n",
    "            temp = {}\n",
    "            for u in TAGS:\n",
    "                temp[u] = forward[j][u] * backwards[j][u]\n",
    "            YSTAR.append(max(temp,key=temp.get))\n",
    "        YSTAR.append(\"STOP\")\n",
    "        MASSIVE_YSTAR.append(YSTAR)\n",
    "    MASSIVE_YSTAR = process_results(MASSIVE_YSTAR,sentences)\n",
    "    return MASSIVE_YSTAR\n",
    "\n",
    "#part 4\n",
    "\n",
    "parsedtrainData = read_file_train(training_file)\n",
    "modifiedData = modify_train_set(3, parsedtrainData)\n",
    "stsp_train = (mod_data_for_transition(modifiedData, \"train\"))\n",
    "huge_count_y_x, huge_count_y = built_dict(modifiedData)\n",
    "huge_emission = calculate_emmision(huge_count_y_x, huge_count_y)\n",
    "huge_count_yi_1_yi, huge_count_yi_1 = build_dict_transition(stsp_train)\n",
    "huge_transition = calculate_transition(huge_count_yi_1_yi, huge_count_yi_1)  # print(huge_transition)\n",
    "parsedtestData = read_file_test(testing_file)\n",
    "stsp_test = mod_data_for_transition(parsedtestData, \"test\")\n",
    "modifiedTestdata = mod_testdata(stsp_test, modifiedData)\n",
    "YSTAR = max_marginal(modifiedTestdata,huge_transition,huge_emission,alpha_base(huge_transition))\n",
    "write_file(output_path, output_file_name, YSTAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# File Paths ####################\n",
    "output_path =   \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\\"\n",
    "output_file_name = \"dev.p5.out\"\n",
    "training_file = \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\Term 6\\\\Machine Learning\\\\Project\\\\EN\\\\train\"\n",
    "testing_file =  \"C:\\\\Users\\\\Lenovo\\\\Google Drive\\\\Courses\\\\Term 6\\Machine Learning\\\\Project\\\\EN\\\\dev.in\"\n",
    "\n",
    "################# Helper Functions ####################\n",
    "def read_file(file_path):\n",
    "    \n",
    "    file = open(file_path,'r',encoding=\"utf8\")\n",
    "    parsed_file = []\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if(line == ''): \n",
    "            break\n",
    "        elif(line == '\\n'):\n",
    "            parsed_file.append((None, None))\n",
    "            continue\n",
    "        else: \n",
    "            temp = line.replace('\\n','').split(' ')\n",
    "            parsed_file.append(tuple(temp)) #return a tuple of word and its tag\n",
    "      \n",
    "    return parsed_file\n",
    "\n",
    "def smartify(file_path):\n",
    "    training_data = read_file(file_path)\n",
    "    smart_data = {} #smart_data is basically training data but in a smart format \n",
    "                    #smart format is {tag:{word:frequency}}\n",
    "                    #example: {'O': {'We': 83, 'were': 125, 'then': 15, 'charged': 2, 'for': 279}\n",
    "    for word_tag_pair in training_data:\n",
    "        \n",
    "        if word_tag_pair==(None,None): #annoying case -.-\n",
    "            continue \n",
    "        \n",
    "        #if tag not in smart_data yet, add it \n",
    "        if not word_tag_pair[1] in smart_data.keys():\n",
    "            smart_data[word_tag_pair[1]]={}\n",
    "        \n",
    "        # if word not in smart_data[tag] yet, add it\n",
    "        if not word_tag_pair[0] in smart_data[word_tag_pair[1]].keys():\n",
    "            smart_data[word_tag_pair[1]][word_tag_pair[0]] = 1\n",
    "        \n",
    "        #increase frequency for each word-tag pair if seen\n",
    "        smart_data[word_tag_pair[1]][word_tag_pair[0]] = smart_data[word_tag_pair[1]][word_tag_pair[0]] +1\n",
    "    return smart_data\n",
    "\n",
    "def modify_training_set(training_data, k):\n",
    "    print(\"Modifying training data to #UNK#...\")\n",
    "    #input: smart_data\n",
    "    count = 0 #not sure if we will need number of words replaced by unk, but lets keep count anyway\n",
    "    \n",
    "    remove_words = []\n",
    "    training_data = training_data\n",
    "    for tag, word in training_data.items():\n",
    "        for specific_word, f in list(word.items()):\n",
    "            if (f<k):\n",
    "                count +=f\n",
    "#                 print(specific_word,f)\n",
    "                #replace with #unk#\n",
    "                word.pop(specific_word)\n",
    "                remove_words.append((tag,specific_word))\n",
    "        word[\"#UNK#\"] = count\n",
    "\n",
    "    return training_data\n",
    "\n",
    "import math\n",
    "def emission(file_path):\n",
    "    print(\"Reading input file...\")\n",
    "    \n",
    "    emissions = {}\n",
    "    training_data = smartify(file_path)\n",
    "    training_data = modify_training_set(training_data, k=7) #modify training data \n",
    "    \n",
    "    print(\"Estimating emission probabilities...\")\n",
    "    for tag, word in training_data.items():\n",
    "#         print(word.values(),\"\\n\")\n",
    "        count_y = sum(word.values())\n",
    "#         print (count_y)\n",
    "        for specific_word, f in word.items():\n",
    "#             print(specific_word,f)\n",
    "            temp_tup = (specific_word,tag)\n",
    "#             print(specific_word, f)\n",
    "            emissions[temp_tup] = float(f/(count_y))\n",
    "\n",
    "\n",
    "    return emissions\n",
    "\n",
    "def read_test_file(file_path):\n",
    "    print(\"Reading test file...\")\n",
    "    file = open(file_path,'r',encoding=\"utf8\")\n",
    "    parsed_file = []\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if(line == ''):\n",
    "            break\n",
    "        if(line == '\\n'):\n",
    "            parsed_file.append(None)\n",
    "            continue\n",
    "        parsed_file.append(line.replace('\\n',''))\n",
    "    return parsed_file\n",
    "\n",
    "def clean_up_test_data(test_data, emissions_dict):\n",
    "    \n",
    "    test_data = test_data\n",
    "#     print(\"Original test data:\",len(test_data))\n",
    "    \n",
    "    training_data = []\n",
    "    list_of_replaced_words = []\n",
    "    for tup in emissions_dict.keys():\n",
    "        training_data.append(tup[0])\n",
    "\n",
    "    #compare training and test data and replace non-common words with #UNK#\n",
    "    for word in test_data:\n",
    "        if word =='START' or word=='STOP':\n",
    "            continue\n",
    "        elif word not in training_data:\n",
    "            list_of_replaced_words.append(word) #we might need this for testing if the function is working\n",
    "            index = test_data.index(word)\n",
    "            test_data[index] = \"#UNK#\"\n",
    "\n",
    "\n",
    "    return test_data\n",
    "\n",
    "def construct_sentences(file_path):\n",
    "    file = open(file_path,'r',encoding = \"utf8\")\n",
    "    sentences = []\n",
    "    sentence = ['START']\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if(line == ''):\n",
    "            break\n",
    "        if(line == '\\n'):\n",
    "            sentence.append('STOP')\n",
    "            sentences.append(sentence)\n",
    "            sentence = ['START']\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(line.replace('\\n',''))\n",
    "            \n",
    "    return sentences  \n",
    "\n",
    "################# Estimating Transition Parameters ####################\n",
    "def prepare_input_data_for_transition_paramaters(file_path):\n",
    "    print(\"Estimating transition parameters...\")\n",
    "    input_data = (read_file(file_path))\n",
    "    input_data = [(\"START\",None)]+input_data\n",
    "    \n",
    "    #wherever (None, None) is seen, instert a STOP and START\n",
    "    for tup in input_data:\n",
    "        if (tup == (None,None)):\n",
    "            index = input_data.index(tup)\n",
    "            #add a stop\n",
    "            input_data[index] = (\"STOP\",None)\n",
    "            input_data = input_data[:index+1] + [(\"START\", None)]+ input_data[index+1:]\n",
    "            \n",
    "    #take care of the ending of the file\n",
    "    input_data.pop(-2)\n",
    "    return input_data \n",
    "\n",
    "def estimate_transition_parameters(file_path):\n",
    "    input_data = prepare_input_data_for_transition_paramaters(file_path)\n",
    "    \n",
    "    emission_pairs = {}\n",
    "    yi_1_count = {}\n",
    "    \n",
    "    \n",
    "    for i in range (len(input_data)-1):\n",
    "        yi_1 = input_data[i][1]\n",
    "        yi = input_data[i+1][1]\n",
    "        \n",
    "        temp_tup = (yi_1,yi)\n",
    "        if not temp_tup in emission_pairs.keys():\n",
    "            emission_pairs[temp_tup] = 1\n",
    "        else:\n",
    "            emission_pairs[temp_tup]+=1\n",
    "        \n",
    "        if not yi_1 in yi_1_count.keys():\n",
    "            yi_1_count[yi_1] = 1\n",
    "        else: \n",
    "            yi_1_count[yi_1] += 1\n",
    "            \n",
    "            \n",
    "#     print (emission_pairs)\n",
    "#     print (yi_1_count)\n",
    "\n",
    "    transition_params = emission_pairs #this is just for convenience\n",
    "    \n",
    "    for yi_1, count in yi_1_count.items():\n",
    "        \n",
    "        for tup in emission_pairs.keys():\n",
    "            if (tup[0]==yi_1):\n",
    "                transition_params[tup] = float(emission_pairs[tup]/count)\n",
    "    \n",
    "#     print(transition_params)\n",
    "    return(transition_params)\n",
    "\n",
    "################# Improved Viterbi ####################\n",
    "import math\n",
    "import random\n",
    "def viterbi(train_file_path, test_file_path):\n",
    "    \n",
    "    tags = ['O','B-positive','B-neutral','B-negative','I-positive','I-negative','I-neutral']\n",
    "    emission_p = emission(train_file_path) #{('We', 'O'): 0.003423668687868663, ('were', 'O'): 0.005156127541970878, ('then', 'O'): 0.0006187353050365054,}\n",
    "    transition_p = estimate_transition_parameters(train_file_path) #{(None, 'O'): 0.47022696929238983, ('O', 'O'): 0.8601188020790363, ('O', None): 0.07623133404834585,}\n",
    "    sentences = construct_sentences(test_file_path)\n",
    "    predictions = []\n",
    "    \n",
    "    print(\"Cleaning up test file to replace unseen words with #UNK#...\")\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i]=clean_up_test_data(sentences[i],emission_p)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        dp_table = [{},{}]\n",
    "        for tag in tags:\n",
    "            if((None,tag) in transition_p.keys()):                \n",
    "                a_i_j = transition_p[(None,tag)] #transition probability from 'START'to tag\n",
    "            else:\n",
    "                a_i_j = 0\n",
    "                            \n",
    "            if( (sentence[1],tag) in emission_p.keys()):                \n",
    "                b_j = emission_p[(sentence[1],tag)] #probability that tag emits word\n",
    "            else:\n",
    "                b_j = 0\n",
    "            dp_table[1][tag] = {'p': a_i_j*b_j, 'previous':None}\n",
    "\n",
    "        for t in range(2,len(sentence)-1):\n",
    "            dp_table.append({})\n",
    "            transition_probabilities = {}\n",
    "\n",
    "            for tag in tags:\n",
    "                for prev_tag in tags:\n",
    "\n",
    "                    if((prev_tag,tag) in transition_p.keys()):                           \n",
    "                        a_i_j = transition_p[(prev_tag,tag)] #transition probability from 'START'to tag\n",
    "                    else:\n",
    "                        a_i_j = 0\n",
    "\n",
    "                    if(prev_tag in dp_table[t-1].keys()):\n",
    "                        t_p = dp_table[t-1][prev_tag]['p']*a_i_j\n",
    "                        transition_probabilities[(prev_tag,tag)]=t_p\n",
    "\n",
    "                max_t_p = max(transition_probabilities.values())\n",
    "                \n",
    "                for key in transition_probabilities.keys():\n",
    "                    if(transition_probabilities[key]==max_t_p):\n",
    "                        argmax_t_p = key\n",
    "                \n",
    "                if((sentence[t],tag) in emission_p.keys()):\n",
    "                    if(sentence[t]==\"#UNK#\"):\n",
    "                        tag = 'O'\n",
    "                        \n",
    "                    b_j = emission_p[(sentence[t],tag)] #probability that tag emits word\n",
    "                else:\n",
    "                    b_j = 0\n",
    "                dp_table[t][tag] = {'p':max_t_p*b_j, 'previous':argmax_t_p[0]}\n",
    "\n",
    "        opt = []\n",
    "        pmax = max(probabilities['p'] for probabilities in dp_table[-1].values())\n",
    "        backward_previous = None\n",
    "        for tag, data in dp_table[-1].items():\n",
    "            if data[\"p\"] == pmax:\n",
    "                opt.append(tag)\n",
    "                prev_tag = tag\n",
    "                break\n",
    "        \n",
    "        for t in range(len(dp_table) - 2, -1, -1):\n",
    "            opt.insert(0, dp_table[t + 1][prev_tag][\"previous\"])\n",
    "            prev_tag = dp_table[t + 1][prev_tag][\"previous\"]\n",
    "        \n",
    "        opt.append(None) #to represent STOP (adding this for convenience and clarity)\n",
    "        predictions.append(list(zip(sentence,opt)))\n",
    "        \n",
    "    return(predictions)\n",
    "\n",
    "def write_dev_out(input_data, output_path):\n",
    "    filename = output_path\n",
    "    with open(filename, 'w',encoding = \"utf8\") as f:\n",
    "        for i in range(len(input_data)):\n",
    "            if i != 0:\n",
    "                f.write(\"\\n\")\n",
    "            for tup in input_data[i]:\n",
    "                if (tup[0] == 'START' or tup[0] == 'STOP'):\n",
    "                    pass\n",
    "                else:\n",
    "                    f.write(tup[0] + ' ' + tup[1] + '\\n')\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "write_dev_out(viterbi(training_file,testing_file),output_path+output_file_name)\n",
    "print(\"Output successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
